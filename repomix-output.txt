This file is a merged representation of the entire codebase, combined into a single document.
Generated by Repomix on: 2025-02-10T00:29:59.692Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
retail_agency/
  ceo/
    tools/
      GetDate.py
      NotionPoster.py
      SlackCommunicator.py
      TaskManager.py
    ceo.py
    instructions.md
  reporting_manager/
    tools/
      utils/
        CustomSQLTool.py
        DynamicOutputParser.py
        prompt_sql.py
        QueryResultManager.py
      DataAnalyzer.py
      ReportGenerator.py
      SQLQueryTool.py
    instructions.md
    reporting_manager.py
  utils/
    db_connection.py
    firebase_db.py
    slack_handler.py
  agency_manifesto.md
  agency.py
  app.py
  main.py
  requirements.txt
  retail_agency_threads.json
  settings.json
.cursorrules
.gitignore
requirements.txt
retail_agency_threads.json
settings.json

================================================================
Files
================================================================

================
File: retail_agency/ceo/tools/GetDate.py
================
from agency_swarm.tools import BaseTool
from pydantic import Field
from datetime import datetime
import pytz

class GetDate(BaseTool):
    """
    A tool for getting real-time date and time information.
    Can provide current date, time, and timezone-specific information.
    """
    
    timezone: str = Field(
        default="UTC",
        description="The timezone to get the date/time in (e.g., 'UTC', 'US/Eastern', 'US/Pacific')"
    )
    format: str = Field(
        default="full",
        description="The format of date/time to return ('full', 'date', 'time', 'datetime')"
    )

    def run(self):
        """
        Gets the current date and time in the specified timezone and format.
        Returns formatted date/time string.
        """
        try:
            # Get timezone
            tz = pytz.timezone(self.timezone)
            current_time = datetime.now(tz)
            
            # Format the output based on request
            if self.format == "full":
                return f"Current {self.timezone} time: {current_time.strftime('%Y-%m-%d %H:%M:%S %Z')}"
            elif self.format == "date":
                return f"Current {self.timezone} date: {current_time.strftime('%Y-%m-%d')}"
            elif self.format == "time":
                return f"Current {self.timezone} time: {current_time.strftime('%H:%M:%S %Z')}"
            elif self.format == "datetime":
                return f"Current {self.timezone}: {current_time.strftime('%Y-%m-%d %H:%M:%S')}"
            else:
                return f"Invalid format specified. Using default: {current_time.strftime('%Y-%m-%d %H:%M:%S %Z')}"
            
        except pytz.exceptions.UnknownTimeZoneError:
            return f"Unknown timezone: {self.timezone}. Using UTC instead: {datetime.now(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S %Z')}"
        except Exception as e:
            return f"Error getting date/time: {str(e)}"

if __name__ == "__main__":
    # Test the tool
    tool = GetDate(timezone="US/Pacific", format="full")
    print(tool.run())
    
    # Test different formats
    print(GetDate(timezone="UTC", format="date").run())
    print(GetDate(timezone="US/Eastern", format="time").run())

================
File: retail_agency/ceo/tools/NotionPoster.py
================
from agency_swarm.tools import BaseTool
from pydantic import Field
import os
from dotenv import load_dotenv
from notion_client import Client
import json
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

# Initialize Notion client with token from environment variables
notion_token = os.getenv("NOTION_TOKEN")
notion_database_id = os.getenv("NOTION_DATABASE_ID")

class NotionPoster(BaseTool):
    """
    A tool for creating and sharing Notion pages.
    Can create new pages in a specified database and return their URLs.
    """
    
    title: str = Field(
        ..., description="The title of the Notion page"
    )
    content: str = Field(
        ..., description="The content to post to Notion. Can include markdown formatting."
    )
    tags: list = Field(
        default=[], description="List of tags/categories for the page"
    )

    def run(self):
        """
        Creates a new page in Notion and returns its URLs.
        Returns both the direct page URL and a sharing link.
        """
        try:
            if not notion_token:
                return "Error: Notion token not found in environment variables."
            
            if not notion_database_id:
                return "Error: Notion database ID not found in environment variables."
            
            logger.info(f"Using Notion token: {notion_token[:10]}...")
            logger.info(f"Using database ID: {notion_database_id}")
            
            # Initialize Notion client
            notion = Client(auth=notion_token)
            
            # Test the connection
            try:
                db = notion.databases.retrieve(notion_database_id)
                logger.info("Successfully connected to Notion database")
                logger.info(f"Database title: {db.get('title', [{}])[0].get('text', {}).get('content', 'Untitled')}")
            except Exception as e:
                logger.error(f"Error connecting to database: {str(e)}")
                return f"Error connecting to database: {str(e)}"
            
            # Create page properties
            properties = {
                "title": {
                    "title": [
                        {
                            "text": {
                                "content": self.title
                            }
                        }
                    ]
                }
            }
            
            # Add tags if provided
            if self.tags:
                properties["Tags"] = {
                    "multi_select": [{"name": tag} for tag in self.tags]
                }
            
            # Create the page
            new_page = notion.pages.create(
                parent={"database_id": notion_database_id},
                properties=properties,
                children=[
                    {
                        "object": "block",
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": [
                                {
                                    "type": "text",
                                    "text": {"content": self.content}
                                }
                            ]
                        }
                    }
                ]
            )
            
            # Get the page ID and create URLs
            page_id = new_page["id"]
            workspace_id = db.get("workspace_id") or db.get("parent", {}).get("workspace_id")
            
            # Format URLs
            direct_url = f"https://notion.so/{page_id.replace('-', '')}"
            sharing_url = f"https://notion.so/{workspace_id}/{page_id.replace('-', '')}" if workspace_id else direct_url
            
            return f"""Successfully created Notion page!
Direct URL: {direct_url}
Sharing URL: {sharing_url}
Database: {db.get('title', [{}])[0].get('text', {}).get('content', 'Untitled')}"""
            
        except Exception as e:
            logger.error(f"Error creating Notion page: {str(e)}", exc_info=True)
            return f"Error creating Notion page: {str(e)}"

if __name__ == "__main__":
    # Test the tool
    tool = NotionPoster(
        title="Test Page with Sharing",
        content="This is a test page created by the NotionPoster tool with sharing enabled.",
        tags=["test", "sharing"]
    )
    print(tool.run())

================
File: retail_agency/ceo/tools/SlackCommunicator.py
================
from agency_swarm.tools import BaseTool
from pydantic import Field
import os
from dotenv import load_dotenv
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError

load_dotenv()

slack_token = os.getenv("SLACK_BOT_TOKEN")

class SlackCommunicator(BaseTool):
    """
    A tool for handling Slack communications, including sending messages, creating threads,
    and managing conversations in a retail management context.
    """
    
    channel_id: str = Field(
        ..., description="The Slack channel ID where the message should be sent"
    )
    message: str = Field(
        ..., description="The message content to be sent"
    )
    thread_ts: str = Field(
        None, description="The timestamp of the parent message to create a thread (optional)"
    )

    def run(self):
        """
        Sends a message to Slack and handles any threading requirements.
        Returns the response from the Slack API.
        """
        try:
            client = WebClient(token=slack_token)
            
            # Prepare message payload
            message_payload = {
                "channel": self.channel_id,
                "text": self.message,
            }
            
            # Add thread_ts if it's a reply
            if self.thread_ts:
                message_payload["thread_ts"] = self.thread_ts
            
            # Send message
            response = client.chat_postMessage(**message_payload)
            
            return f"Message sent successfully. Timestamp: {response['ts']}"
            
        except SlackApiError as e:
            return f"Error sending message: {str(e)}"

if __name__ == "__main__":
    # Test the tool
    tool = SlackCommunicator(
        channel_id="TEST_CHANNEL_ID",
        message="This is a test message from the Retail Management Agency.",
    )
    print(tool.run())

================
File: retail_agency/ceo/tools/TaskManager.py
================
from agency_swarm.tools import BaseTool
from pydantic import Field
import os
from dotenv import load_dotenv
from datetime import datetime
import json

load_dotenv()

class TaskManager(BaseTool):
    """
    A tool for managing and tracking tasks within the retail management agency.
    Handles task creation, updates, and status tracking.
    """
    
    task_id: str = Field(
        None, description="Unique identifier for the task (generated automatically for new tasks)"
    )
    task_type: str = Field(
        ..., description="Type of task (e.g., 'reporting', 'store_operations', 'customer_service')"
    )
    description: str = Field(
        ..., description="Detailed description of the task"
    )
    assigned_to: str = Field(
        ..., description="Agent or team the task is assigned to"
    )
    priority: str = Field(
        "medium", description="Task priority (low, medium, high)"
    )
    status: str = Field(
        "new", description="Current status of the task (new, in_progress, completed, blocked)"
    )

    def run(self):
        """
        Manages task operations including creation, updates, and status changes.
        Returns the updated task information.
        """
        try:
            # Create task data structure
            task_data = {
                "task_id": self.task_id or f"TASK_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "task_type": self.task_type,
                "description": self.description,
                "assigned_to": self.assigned_to,
                "priority": self.priority,
                "status": self.status,
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat()
            }
            
            # In a real implementation, this would interact with a database
            # For now, we'll just return the task data
            return json.dumps(task_data, indent=2)
            
        except Exception as e:
            return f"Error managing task: {str(e)}"

if __name__ == "__main__":
    # Test the tool
    tool = TaskManager(
        task_type="reporting",
        description="Generate monthly sales report for Q1 2024",
        assigned_to="reporting_manager",
        priority="high"
    )
    print(tool.run())

================
File: retail_agency/ceo/ceo.py
================
from agency_swarm import Agent
from .tools.SlackCommunicator import SlackCommunicator
from .tools.TaskManager import TaskManager
from .tools.GetDate import GetDate
from .tools.NotionPoster import NotionPoster
import json
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class CEO(Agent):
    def __init__(self):
        super().__init__(
            name="CEO",
            description=(
                "Retail Management Agency CEO responsible for client communication, "
                "task delegation, and coordinating with specialized agents like the ReportingManager "
                "for data analysis and insights."
            ),
            instructions="./instructions.md",
            tools=[SlackCommunicator, TaskManager, GetDate, NotionPoster],
            temperature=0.5,
            max_prompt_tokens=25000
        )
        
    def _create_notion_report(self, title, data, insights=None, request_context=None):
        """Create a detailed Notion report with data and insights."""
        try:
            # Format the content
            content = f"## Request Context\n{request_context}\n\n" if request_context else ""
            content += f"## Generated at\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            
            # Add data section
            if isinstance(data, str):
                content += f"## Data\n{data}\n\n"
            elif isinstance(data, dict):
                content += "## Data\n"
                for key, value in data.items():
                    content += f"### {key}\n{value}\n\n"
            elif isinstance(data, list):
                content += "## Data\n"
                for item in data:
                    content += f"- {item}\n"
                content += "\n"
                
            # Add insights section
            if insights:
                content += "## Key Insights\n"
                if isinstance(insights, list):
                    for insight in insights:
                        content += f"- {insight}\n"
                else:
                    content += f"{insights}\n"
            
            # Create Notion page
            notion_poster = NotionPoster(
                title=title,
                content=content,
                tags=["report", "data_analysis"]
            )
            result = notion_poster.run()
            
            # Extract URL from result
            if "Direct URL:" in result:
                url = result.split("Direct URL:")[1].split()[0]
                return url
                
            return result
            
        except Exception as e:
            logger.error(f"Error creating Notion report: {str(e)}")
            return None
            
    def _format_data_response(self, data, notion_url=None):
        """Format the response message with data and optional Notion link."""
        response = ""
        
        # Add summary section
        if isinstance(data, dict) and "summary" in data:
            response += f"Summary:\n{data['summary']}\n\n"
            
        # Add key metrics if available
        if isinstance(data, dict) and "key_metrics" in data:
            response += "Key Metrics:\n"
            for metric, value in data["key_metrics"].items():
                response += f"- {metric}: {value}\n"
            response += "\n"
            
        # Add Notion link if available
        if notion_url:
            response += f"View the full report here: {notion_url}\n"
            
        return response.strip()

================
File: retail_agency/ceo/instructions.md
================
# Agent Role

You are the CEO of a retail management agency, serving as the primary point of contact for all user communications. Your role is to understand user requests, analyze their needs, and efficiently delegate tasks to the appropriate specialized agents while maintaining professional and helpful communication.

# Goals

1. **Request Analysis and Delegation**
   - Analyze incoming requests to understand their requirements
   - For data/analytics requests:
     * Delegate to ReportingManager
     * Wait for and process their response
     * Post detailed results to Notion if:
       - The data contains more than 10 rows
       - The user specifically requests a link
       - The response includes charts or complex tables
   - For other requests, handle directly with appropriate tools

2. **Data Request Handling**
   - When receiving data-related questions:
     * Forward the query to ReportingManager
     * Include specific parameters like date ranges, metrics, and grouping requirements
     * Process the response:
       - For small datasets: Display directly in the conversation
       - For large datasets: Create a Notion page with full results and share the link
     * Always include key insights and summary metrics in the conversation

3. **Notion Integration**
   - Create well-structured Notion pages for:
     * Complex data analysis results
     * Reports with multiple tables or charts
     * Historical data comparisons
     * Detailed breakdowns requested by users
   - Include in each Notion page:
     * Clear title describing the analysis
     * Date and time of the request
     * Summary of key findings
     * Detailed data tables or results
     * Any relevant visualizations
     * Source of the data and parameters used

4. **Quality Assurance**
   - Verify data completeness before sharing
   - Ensure proper formatting of results
   - Add context and insights to raw data
   - Follow up on outstanding requests

# Process Workflow

1. **Initial Request Processing**
   - Acknowledge receipt of request
   - Analyze request type:
     * If data-related → Engage ReportingManager
     * If operational → Handle directly
     * If complex → Break down into subtasks

2. **Data Request Handling**
   - For data requests:
     a. Forward to ReportingManager with clear parameters
     b. Process the response:
        * Small results (≤10 rows) → Display in conversation
        * Large results → Create Notion page
        * Include summary in conversation regardless
     c. Add insights and context to raw data
     d. Share results appropriately

3. **Notion Page Creation**
   - When creating Notion pages:
     a. Use clear, descriptive titles
     b. Include request context and parameters
     c. Structure data logically
     d. Add summary insights
     e. Format for readability
     f. Share link in conversation

4. **Follow-up Actions**
   - Monitor request completion
   - Verify data accuracy
   - Ask for clarification if needed
   - Provide status updates

# Communication Guidelines

1. **Response Format**
   - For direct responses:
     ```
     [Summary of findings]
     [Key metrics or small data tables]
     [Insights or recommendations]
     [Notion link if applicable]
     ```

2. **Data Presentation**
   - Small datasets:
     ```
     Here are the results:
     [Data table or metrics]
     Key insights:
     - [Insight 1]
     - [Insight 2]
     ```
   - Large datasets:
     ```
     I've analyzed the data and created a detailed report.
     
     Summary:
     [Key metrics and highlights]
     
     View the full report here: [Notion link]
     ```

3. **Status Updates**
   - When delegating to ReportingManager:
     ```
     I'm working with our Reporting Manager to analyze [specific request].
     I'll provide [results/updates] shortly.
     ```

Remember: Your role is to ensure efficient communication flow and proper handling of all requests, especially data-related ones. Always provide context and insights along with raw data, and use Notion strategically for complex or detailed results.

================
File: retail_agency/reporting_manager/tools/utils/CustomSQLTool.py
================
from langchain_community.utilities import SQLDatabase
from langchain_openai import ChatOpenAI
from langchain_community.agent_toolkits import create_sql_agent
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
from langchain_core.prompts import PromptTemplate
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
from typing import Optional, Dict, Any
from dotenv import load_dotenv
import os
import json

# Custom prompt that enforces structured output
CUSTOM_SQL_PREFIX = """You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.
Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.
You can order the results by a relevant column to return the most interesting examples in the database.
Never query for all the columns from a specific table, only ask for the relevant columns given the question.

You have access to tools for interacting with the database.
Only use the below tools. Only use the information returned by the below tools to construct your final answer.
You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.

DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.

IMPORTANT: When the sql_db_query tool returns results, you must include those exact results in your response.
Your final response MUST be valid JSON in the following format:
{{
    "question": "<the original question asked>",
    "sql_query": "<the SQL query you generated and executed>",
    "sql_result": <the exact results returned by sql_db_query>
}}

For example, if sql_db_query returns "[('John', 100), ('Mary', 200)]", your response should include that exact output:
{{
    "question": "Who are our top customers?",
    "sql_query": "SELECT name, total_spent FROM customers ORDER BY total_spent DESC LIMIT 2",
    "sql_result": [("John", 100), ("Mary", 200)]
}}

If the question does not seem related to the database, return:
{{
    "question": "<the original question>",
    "sql_query": null,
    "sql_result": "I don't know"
}}
"""

def create_structured_sql_agent(
    llm: ChatOpenAI,
    db: SQLDatabase,
    verbose: bool = True,
    top_k: int = 10,
    **kwargs: Any
) -> Any:
    """Create a SQL agent that returns structured output."""
    
    # Create the toolkit with Langchain's standard tools
    toolkit = SQLDatabaseToolkit(db=db, llm=llm)

    # Create messages for the prompt
    messages = [
        SystemMessagePromptTemplate.from_template(CUSTOM_SQL_PREFIX),
        HumanMessagePromptTemplate.from_template("{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
    
    # Create prompt
    prompt = ChatPromptTemplate.from_messages(messages).partial(
        dialect=toolkit.dialect,
        top_k=top_k
    )
    
    # Create the agent using Langchain's standard SQL agent creator
    return create_sql_agent(
        llm=llm,
        toolkit=toolkit,
        verbose=verbose,
        agent_type="openai-tools",
        prompt=prompt,
        **kwargs
    )

if __name__ == "__main__":
    # Load environment variables
    load_dotenv()
    
    # Initialize database connection
    db = SQLDatabase.from_uri(
        f"postgresql+psycopg2://{os.getenv('CLOUD_DB_USER')}:{os.getenv('CLOUD_DB_PASS')}@{os.getenv('CLOUD_DB_HOST')}:{os.getenv('CLOUD_DB_PORT')}/{os.getenv('CLOUD_DB_NAME')}",
        schema=os.getenv('CLOUD_SCHEMA')
    )
    
    # Initialize LLM
    llm = ChatOpenAI(temperature=0, model="gpt-4")
    
    # Create the agent
    agent = create_structured_sql_agent(
        llm=llm,
        db=db,
        verbose=True  # This will show the agent's thought process
    )
    
    # Test query
    query = "Get me a list of all inventory with a low stock level below 5"
    
    print(f"\nExecuting query: {query}")
    response = agent.invoke({"input": query})
    
    # Try to parse and format the JSON response
    try:
        if isinstance(response, dict) and 'output' in response:
            result = json.loads(response['output'])
            print("\nFormatted Response:")
            print("==================")
            print(f"Question: {result['question']}")
            print(f"SQL Query: {result['sql_query']}")
            print("\nResults:")
            print(result['sql_result'])
        else:
            print("\nRaw Response:")
            print(response)
    except Exception as e:
        print("\nRaw Response:")
        print(response)
        print(f"\nError parsing response: {str(e)}")

================
File: retail_agency/reporting_manager/tools/utils/DynamicOutputParser.py
================
from typing import List, Dict, Any
import json
import re
import pandas as pd
from io import StringIO
from datetime import datetime

class ResultParser:
    def parse(self, text: str) -> List[Dict[str, Any]]:
        raise NotImplementedError("Parser must implement the parse method.")

class JSONResultParser(ResultParser):
    def parse(self, text: str) -> List[Dict[str, Any]]:
        try:
            result = json.loads(text)
            if isinstance(result, dict):
                return [result]
            elif isinstance(result, list):
                return result
            else:
                return []
        except json.JSONDecodeError:
            return []

class CSVResultParser(ResultParser):
    def parse(self, text: str) -> List[Dict[str, Any]]:
        try:
            df = pd.read_csv(StringIO(text))
            return df.to_dict(orient='records')
        except Exception:
            return []

class TableResultParser(ResultParser):
    def parse(self, text: str) -> List[Dict[str, Any]]:
        try:
            lines = [line.strip() for line in text.splitlines() if line.strip()]
            if len(lines) < 3:
                return []
            # Assume the first line contains headers and the second is a separator
            headers = [h.strip() for h in lines[0].split('|') if h.strip()]
            data_lines = lines[2:]
            result = []
            for line in data_lines:
                if '|' not in line:
                    continue
                values = [v.strip() for v in line.split('|') if v.strip()]
                if len(values) != len(headers):
                    continue
                record = {headers[i].lower().replace(' ', '_'): values[i] for i in range(len(headers))}
                result.append(record)
            return result
        except Exception:
            return []

class NumberedListResultParser(ResultParser):
    def parse(self, text: str) -> List[Dict[str, Any]]:
        try:
            pattern = r'\d+\.(.*?)(?=\n\d+\.|$)'
            matches = re.findall(pattern, text, re.DOTALL)
            result = []
            for match in matches:
                record = {"text": match.strip()}
                result.append(record)
            return result
        except Exception:
            return []

class KeyValueResultParser(ResultParser):
    def parse(self, text: str) -> List[Dict[str, Any]]:
        try:
            items = []
            current_item = {}
            for line in text.splitlines():
                line = line.strip()
                if not line:
                    if current_item:
                        items.append(current_item)
                        current_item = {}
                    continue
                match = re.match(r'([^:]+):\s*(.+)', line)
                if match:
                    key = match.group(1).strip().lower().replace(' ', '_')
                    value = match.group(2).strip()
                    current_item[key] = value
            if current_item:
                items.append(current_item)
            return items
        except Exception:
            return []

class DynamicOutputParser:
    def __init__(self):
        self.parsers = [
            JSONResultParser(),
            CSVResultParser(),
            TableResultParser(),
            NumberedListResultParser(),
            KeyValueResultParser()
        ]
    
    def parse(self, text: str) -> List[Dict[str, Any]]:
        for parser in self.parsers:
            result = parser.parse(text)
            if result and isinstance(result, list) and len(result) > 0:
                for record in result:
                    if isinstance(record, dict) and 'timestamp' not in record:
                        record['timestamp'] = datetime.now().isoformat()
                return result
        return []

================
File: retail_agency/reporting_manager/tools/utils/prompt_sql.py
================
# flake8: noqa

SQL_PREFIX = """You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.
Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.
You can order the results by a relevant column to return the most interesting examples in the database.
Never query for all the columns from a specific table, only ask for the relevant columns given the question.
You have access to tools for interacting with the database.
Only use the below tools. Only use the information returned by the below tools to construct your final answer.
You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.

DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.

If the question does not seem related to the database, just return "I don't know" as the answer.
"""

SQL_SUFFIX = """Begin!

Question: {input}
Thought: I should look at the tables in the database to see what I can query.  Then I should query the schema of the most relevant tables.
{agent_scratchpad}"""

SQL_FUNCTIONS_SUFFIX = """I should look at the tables in the database to see what I can query.  Then I should query the schema of the most relevant tables."""

================
File: retail_agency/reporting_manager/tools/utils/QueryResultManager.py
================
import pandas as pd
import json
from datetime import datetime
from pathlib import Path
import logging
import uuid
from typing import Union, Dict, List, Any

logger = logging.getLogger(__name__)

class QueryResultManager:
    """Manages storage and retrieval of SQL query results."""
    
    def __init__(self, base_path: str = "./query_results"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
        
    def save_result(self, data: Any, metadata: Dict = None) -> str:
        """Save query result and return a reference ID."""
        try:
            # Generate unique ID
            result_id = str(uuid.uuid4())
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Create directory for this result
            result_dir = self.base_path / result_id
            result_dir.mkdir(exist_ok=True)
            
            # Save metadata
            meta = {
                "id": result_id,
                "timestamp": timestamp,
                "size": len(data) if hasattr(data, '__len__') else 1,
                **(metadata or {})
            }
            
            with open(result_dir / "metadata.json", "w") as f:
                json.dump(meta, f, indent=2)
            
            # Save data based on type
            if isinstance(data, (list, tuple)) and len(data) > 0:
                if isinstance(data[0], (list, tuple, dict)):
                    # Convert to DataFrame for structured data
                    df = pd.DataFrame(data)
                    df.to_csv(result_dir / "data.csv", index=False)
                    format_type = "csv"
                else:
                    # Save as JSON for simple lists
                    with open(result_dir / "data.json", "w") as f:
                        json.dump(data, f, indent=2)
                    format_type = "json"
            else:
                # Save as JSON for other types
                with open(result_dir / "data.json", "w") as f:
                    json.dump(data, f, indent=2)
                format_type = "json"
            
            return {
                "result_id": result_id,
                "format": format_type,
                "path": str(result_dir),
                "size": meta["size"],
                "url": f"/api/query_results/{result_id}"
            }
            
        except Exception as e:
            logger.error(f"Error saving result: {e}")
            raise
            
    def get_result(self, result_id: str) -> Dict[str, Any]:
        """Retrieve saved result by ID."""
        try:
            result_dir = self.base_path / result_id
            
            # Load metadata
            with open(result_dir / "metadata.json", "r") as f:
                metadata = json.load(f)
            
            # Load data based on saved format
            if (result_dir / "data.csv").exists():
                data = pd.read_csv(result_dir / "data.csv").to_dict('records')
            else:
                with open(result_dir / "data.json", "r") as f:
                    data = json.load(f)
            
            return {
                "metadata": metadata,
                "data": data
            }
            
        except Exception as e:
            logger.error(f"Error retrieving result {result_id}: {e}")
            raise
            
    def get_summary(self, result_id: str) -> Dict[str, Any]:
        """Get summary of saved result without loading full data."""
        try:
            result_dir = self.base_path / result_id
            with open(result_dir / "metadata.json", "r") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error getting summary for {result_id}: {e}")
            raise

================
File: retail_agency/reporting_manager/tools/DataAnalyzer.py
================
from agency_swarm.tools import BaseTool
from pydantic import Field, ConfigDict
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
from typing import Optional, List, Dict, Union, Any
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataAnalyzer(BaseTool):
    """
    A tool for analyzing retail data using advanced statistical and machine learning methods.
    Supports various types of analysis including trend analysis, forecasting, and anomaly detection.
    """
    
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    data: Union[pd.DataFrame, Dict[str, Any]] = Field(
        ..., description="Data to analyze. Can be a DataFrame or a dictionary of data."
    )
    analysis_type: str = Field(
        ..., description="Type of analysis to perform (trend, forecast, segment, anomaly, correlation)"
    )
    time_column: Optional[str] = Field(
        None, description="Name of the time column for time series analysis"
    )
    target_column: Optional[str] = Field(
        None, description="Name of the target column to analyze"
    )
    feature_columns: Optional[List[str]] = Field(
        None, description="List of feature columns for analysis"
    )
    params: Optional[dict] = Field(
        default={}, description="Additional parameters for the analysis"
    )

    def run(self):
        """
        Performs the requested analysis on the provided data.
        Returns the analysis results in a structured format.
        """
        try:
            # Convert data to DataFrame if it's a dictionary
            if isinstance(self.data, dict):
                if 'values' in self.data:
                    df = pd.DataFrame(self.data['values'])
                else:
                    df = pd.DataFrame(self.data)
            else:
                df = self.data

            # Perform the requested analysis
            if self.analysis_type == "trend":
                return self._analyze_trends(df)
            elif self.analysis_type == "forecast":
                return self._generate_forecast(df)
            elif self.analysis_type == "segment":
                return self._segment_data(df)
            elif self.analysis_type == "anomaly":
                return self._detect_anomalies(df)
            elif self.analysis_type == "correlation":
                return self._analyze_correlations(df)
            else:
                raise ValueError(f"Unsupported analysis type: {self.analysis_type}")

        except Exception as e:
            logger.error(f"Error analyzing data: {str(e)}", exc_info=True)
            return {"error": str(e)}

    def _analyze_trends(self, df: pd.DataFrame) -> dict:
        """
        Analyzes trends in the data using various statistical methods.
        """
        try:
            if not self.time_column or not self.target_column:
                raise ValueError("Time and target columns must be specified for trend analysis")

            # Convert time column to datetime if needed
            df[self.time_column] = pd.to_datetime(df[self.time_column])
            df = df.sort_values(self.time_column)

            # Calculate various trend metrics
            results = {
                "overall_trend": self._calculate_overall_trend(df),
                "seasonal_patterns": self._identify_seasonal_patterns(df),
                "growth_rates": self._calculate_growth_rates(df),
                "summary_statistics": self._calculate_summary_stats(df)
            }

            return results

        except Exception as e:
            logger.error(f"Error in trend analysis: {str(e)}", exc_info=True)
            return {"error": str(e)}

    def _generate_forecast(self, df: pd.DataFrame) -> dict:
        """
        Generates forecasts using various time series forecasting methods.
        """
        try:
            if not self.time_column or not self.target_column:
                raise ValueError("Time and target columns must be specified for forecasting")

            # Convert time column to datetime if needed
            df[self.time_column] = pd.to_datetime(df[self.time_column])
            df = df.sort_values(self.time_column)

            # Get forecast parameters
            forecast_periods = self.params.get('forecast_periods', 30)
            
            # Perform exponential smoothing forecast
            model = ExponentialSmoothing(
                df[self.target_column],
                seasonal_periods=self.params.get('seasonal_periods', 7),
                trend='add',
                seasonal='add'
            )
            fitted_model = model.fit()
            forecast = fitted_model.forecast(forecast_periods)
            
            # Calculate confidence intervals
            residuals = fitted_model.resid
            std_resid = np.std(residuals)
            conf_int = 1.96 * std_resid  # 95% confidence interval
            
            return {
                "forecast": forecast.tolist(),
                "confidence_interval": conf_int,
                "model_metrics": {
                    "aic": fitted_model.aic,
                    "bic": fitted_model.bic,
                    "mse": np.mean(residuals ** 2)
                }
            }

        except Exception as e:
            logger.error(f"Error in forecasting: {str(e)}", exc_info=True)
            return {"error": str(e)}

    def _segment_data(self, df: pd.DataFrame) -> dict:
        """
        Segments the data using clustering and dimensionality reduction techniques.
        """
        try:
            if not self.feature_columns:
                raise ValueError("Feature columns must be specified for segmentation")

            # Prepare features for clustering
            features = df[self.feature_columns]
            
            # Handle missing values
            features = features.fillna(features.mean())
            
            # Standardize features
            scaler = StandardScaler()
            scaled_features = scaler.fit_transform(features)
            
            # Perform PCA for dimensionality reduction
            pca = PCA(n_components=min(len(self.feature_columns), 3))
            reduced_features = pca.fit_transform(scaled_features)
            
            # Perform clustering
            n_clusters = self.params.get('n_clusters', 3)
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            clusters = kmeans.fit_predict(scaled_features)
            
            # Calculate cluster statistics
            cluster_stats = []
            for i in range(n_clusters):
                cluster_data = features[clusters == i]
                stats = {
                    "size": len(cluster_data),
                    "percentage": len(cluster_data) / len(features) * 100,
                    "mean": cluster_data.mean().to_dict(),
                    "std": cluster_data.std().to_dict()
                }
                cluster_stats.append(stats)
            
            return {
                "n_clusters": n_clusters,
                "cluster_assignments": clusters.tolist(),
                "cluster_statistics": cluster_stats,
                "explained_variance_ratio": pca.explained_variance_ratio_.tolist()
            }

        except Exception as e:
            logger.error(f"Error in data segmentation: {str(e)}", exc_info=True)
            return {"error": str(e)}

    def _detect_anomalies(self, df: pd.DataFrame) -> dict:
        """
        Detects anomalies in the data using statistical methods.
        """
        try:
            if not self.target_column:
                raise ValueError("Target column must be specified for anomaly detection")

            # Calculate statistical properties
            mean = df[self.target_column].mean()
            std = df[self.target_column].std()
            
            # Define anomaly thresholds (e.g., 3 standard deviations)
            threshold = self.params.get('threshold', 3)
            upper_bound = mean + threshold * std
            lower_bound = mean - threshold * std
            
            # Identify anomalies
            anomalies = df[
                (df[self.target_column] > upper_bound) |
                (df[self.target_column] < lower_bound)
            ]
            
            return {
                "n_anomalies": len(anomalies),
                "anomaly_indices": anomalies.index.tolist(),
                "anomaly_values": anomalies[self.target_column].tolist(),
                "threshold_values": {
                    "upper": upper_bound,
                    "lower": lower_bound,
                    "mean": mean,
                    "std": std
                }
            }

        except Exception as e:
            logger.error(f"Error in anomaly detection: {str(e)}", exc_info=True)
            return {"error": str(e)}

    def _analyze_correlations(self, df: pd.DataFrame) -> dict:
        """
        Analyzes correlations between features in the data.
        """
        try:
            if not self.feature_columns:
                raise ValueError("Feature columns must be specified for correlation analysis")

            # Calculate correlation matrix
            corr_matrix = df[self.feature_columns].corr()
            
            # Find strong correlations
            threshold = self.params.get('correlation_threshold', 0.7)
            strong_correlations = []
            
            for i in range(len(self.feature_columns)):
                for j in range(i + 1, len(self.feature_columns)):
                    correlation = corr_matrix.iloc[i, j]
                    if abs(correlation) >= threshold:
                        strong_correlations.append({
                            "feature1": self.feature_columns[i],
                            "feature2": self.feature_columns[j],
                            "correlation": correlation
                        })
            
            return {
                "correlation_matrix": corr_matrix.to_dict(),
                "strong_correlations": strong_correlations,
                "summary": {
                    "n_strong_correlations": len(strong_correlations),
                    "max_correlation": corr_matrix.max().max(),
                    "min_correlation": corr_matrix.min().min()
                }
            }

        except Exception as e:
            logger.error(f"Error in correlation analysis: {str(e)}", exc_info=True)
            return {"error": str(e)}

    def _calculate_overall_trend(self, df: pd.DataFrame) -> dict:
        """
        Calculates the overall trend in the target variable.
        """
        values = df[self.target_column].values
        x = np.arange(len(values))
        z = np.polyfit(x, values, 1)
        slope = z[0]
        
        return {
            "direction": "increasing" if slope > 0 else "decreasing",
            "slope": slope,
            "start_value": values[0],
            "end_value": values[-1],
            "percent_change": ((values[-1] - values[0]) / values[0]) * 100
        }

    def _identify_seasonal_patterns(self, df: pd.DataFrame) -> dict:
        """
        Identifies seasonal patterns in the data.
        """
        # Calculate daily, weekly, and monthly averages
        df['day_of_week'] = df[self.time_column].dt.day_name()
        df['month'] = df[self.time_column].dt.month_name()
        
        daily_avg = df.groupby('day_of_week')[self.target_column].mean().to_dict()
        monthly_avg = df.groupby('month')[self.target_column].mean().to_dict()
        
        return {
            "daily_pattern": daily_avg,
            "monthly_pattern": monthly_avg,
            "peak_day": max(daily_avg.items(), key=lambda x: x[1])[0],
            "peak_month": max(monthly_avg.items(), key=lambda x: x[1])[0]
        }

    def _calculate_growth_rates(self, df: pd.DataFrame) -> dict:
        """
        Calculates various growth rates in the data.
        """
        # Calculate period-over-period growth rates
        daily_growth = df[self.target_column].pct_change().mean() * 100
        weekly_growth = df[self.target_column].pct_change(7).mean() * 100
        monthly_growth = df[self.target_column].pct_change(30).mean() * 100
        
        return {
            "daily_growth_rate": daily_growth,
            "weekly_growth_rate": weekly_growth,
            "monthly_growth_rate": monthly_growth
        }

    def _calculate_summary_stats(self, df: pd.DataFrame) -> dict:
        """
        Calculates summary statistics for the target variable.
        """
        return {
            "mean": df[self.target_column].mean(),
            "median": df[self.target_column].median(),
            "std": df[self.target_column].std(),
            "min": df[self.target_column].min(),
            "max": df[self.target_column].max(),
            "q1": df[self.target_column].quantile(0.25),
            "q3": df[self.target_column].quantile(0.75)
        }

if __name__ == "__main__":
    # Test the tool with sample data
    dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')
    np.random.seed(42)
    
    # Generate sample data
    sales = np.random.normal(1000, 100, len(dates))
    sales = sales + np.sin(np.arange(len(dates)) * 2 * np.pi / 7) * 100  # Weekly seasonality
    sales = sales + np.sin(np.arange(len(dates)) * 2 * np.pi / 365) * 200  # Yearly seasonality
    sales = sales + np.arange(len(dates)) * 0.5  # Upward trend
    
    # Create test DataFrame
    test_data = pd.DataFrame({
        'date': dates,
        'sales': sales,
        'customers': np.random.normal(500, 50, len(dates)),
        'items_per_order': np.random.normal(3, 0.5, len(dates)),
        'average_price': np.random.normal(50, 5, len(dates))
    })
    
    # Test trend analysis
    analyzer = DataAnalyzer(
        data=test_data,
        analysis_type="trend",
        time_column="date",
        target_column="sales"
    )
    print("\nTrend Analysis Results:")
    print(analyzer.run())
    
    # Test forecasting
    analyzer = DataAnalyzer(
        data=test_data,
        analysis_type="forecast",
        time_column="date",
        target_column="sales",
        params={'forecast_periods': 30, 'seasonal_periods': 7}
    )
    print("\nForecasting Results:")
    print(analyzer.run())
    
    # Test segmentation
    analyzer = DataAnalyzer(
        data=test_data,
        analysis_type="segment",
        feature_columns=['sales', 'customers', 'items_per_order', 'average_price'],
        params={'n_clusters': 3}
    )
    print("\nSegmentation Results:")
    print(analyzer.run())
    
    # Test anomaly detection
    analyzer = DataAnalyzer(
        data=test_data,
        analysis_type="anomaly",
        target_column="sales",
        params={'threshold': 3}
    )
    print("\nAnomaly Detection Results:")
    print(analyzer.run())
    
    # Test correlation analysis
    analyzer = DataAnalyzer(
        data=test_data,
        analysis_type="correlation",
        feature_columns=['sales', 'customers', 'items_per_order', 'average_price'],
        params={'correlation_threshold': 0.7}
    )
    print("\nCorrelation Analysis Results:")
    print(analyzer.run())

================
File: retail_agency/reporting_manager/tools/ReportGenerator.py
================
from agency_swarm.tools import BaseTool
from pydantic import Field
import os
from dotenv import load_dotenv
from notion_client import Client
import json
from datetime import datetime
import pandas as pd
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

# Initialize Notion client
notion_token = os.getenv("NOTION_TOKEN")
notion_database_id = os.getenv("NOTION_DATABASE_ID")

class ReportGenerator(BaseTool):
    """
    A tool for generating formatted reports and saving them to Notion.
    Supports various report formats and includes data visualization capabilities.
    """
    
    report_type: str = Field(
        ..., description="Type of report to generate (executive_summary, detailed_analysis, dashboard)"
    )
    data: dict = Field(
        ..., description="The data to include in the report"
    )
    title: str = Field(
        ..., description="Title for the report"
    )
    format_type: str = Field(
        "notion", description="Output format (notion, text, json)"
    )
    tags: list = Field(
        default=[], description="Tags to categorize the report"
    )

    def run(self):
        """
        Generates a formatted report and saves it to Notion.
        Returns the report content and Notion URL.
        """
        try:
            # Generate the report content
            report_content = self._generate_report_content()
            
            # If Notion format is requested, save to Notion
            if self.format_type == "notion":
                return self._save_to_notion(report_content)
            elif self.format_type == "text":
                return self._format_as_text(report_content)
            elif self.format_type == "json":
                return json.dumps(report_content, indent=2)
            else:
                raise ValueError(f"Unsupported format: {self.format_type}")
            
        except Exception as e:
            logger.error(f"Error generating report: {str(e)}", exc_info=True)
            return f"Error generating report: {str(e)}"
    
    def _generate_report_content(self):
        """
        Generates the report content based on the report type.
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        if self.report_type == "executive_summary":
            content = self._generate_executive_summary()
        elif self.report_type == "detailed_analysis":
            content = self._generate_detailed_analysis()
        elif self.report_type == "dashboard":
            content = self._generate_dashboard()
        else:
            raise ValueError(f"Unknown report type: {self.report_type}")
        
        return {
            "title": self.title,
            "type": self.report_type,
            "generated_at": timestamp,
            "content": content
        }
    
    def _generate_executive_summary(self):
        """
        Generates an executive summary with key metrics and insights.
        """
        return {
            "key_metrics": self._extract_key_metrics(),
            "highlights": self._generate_highlights(),
            "recommendations": self._generate_recommendations()
        }
    
    def _generate_detailed_analysis(self):
        """
        Generates a detailed analysis with comprehensive data breakdown.
        """
        return {
            "methodology": self._describe_methodology(),
            "detailed_metrics": self._analyze_detailed_metrics(),
            "trends": self._analyze_trends(),
            "insights": self._generate_insights()
        }
    
    def _generate_dashboard(self):
        """
        Generates a dashboard-style report with key performance indicators.
        """
        return {
            "kpis": self._calculate_kpis(),
            "performance_metrics": self._analyze_performance(),
            "alerts": self._generate_alerts()
        }
    
    def _extract_key_metrics(self):
        """Extract and format key metrics from the data."""
        metrics = {}
        for key, value in self.data.items():
            if isinstance(value, (int, float)):
                metrics[key] = value
            elif isinstance(value, dict) and "value" in value:
                metrics[key] = value["value"]
        return metrics
    
    def _generate_highlights(self):
        """Generate key highlights from the data."""
        highlights = []
        for key, value in self.data.items():
            if isinstance(value, dict) and "highlight" in value:
                highlights.append(value["highlight"])
        return highlights
    
    def _generate_recommendations(self):
        """Generate recommendations based on the data."""
        recommendations = []
        for key, value in self.data.items():
            if isinstance(value, dict) and "recommendation" in value:
                recommendations.append(value["recommendation"])
        return recommendations
    
    def _describe_methodology(self):
        """Describe the methodology used in the analysis."""
        return {
            "data_sources": self.data.get("data_sources", ["Not specified"]),
            "time_period": self.data.get("time_period", "Not specified"),
            "analysis_methods": self.data.get("analysis_methods", ["Not specified"])
        }
    
    def _analyze_detailed_metrics(self):
        """Analyze and format detailed metrics."""
        return {key: value for key, value in self.data.items() 
                if isinstance(value, dict) and "detailed" in value}
    
    def _analyze_trends(self):
        """Analyze trends in the data."""
        return {key: value for key, value in self.data.items() 
                if isinstance(value, dict) and "trend" in value}
    
    def _generate_insights(self):
        """Generate insights from the data."""
        return {key: value for key, value in self.data.items() 
                if isinstance(value, dict) and "insight" in value}
    
    def _calculate_kpis(self):
        """Calculate key performance indicators."""
        return {key: value for key, value in self.data.items() 
                if isinstance(value, dict) and "kpi" in value}
    
    def _analyze_performance(self):
        """Analyze performance metrics."""
        return {key: value for key, value in self.data.items() 
                if isinstance(value, dict) and "performance" in value}
    
    def _generate_alerts(self):
        """Generate alerts based on the data."""
        return {key: value for key, value in self.data.items() 
                if isinstance(value, dict) and "alert" in value}
    
    def _save_to_notion(self, report_content):
        """
        Saves the report to Notion and returns the URL.
        """
        try:
            if not notion_token or not notion_database_id:
                return "Error: Notion credentials not found in environment variables."
            
            # Initialize Notion client
            notion = Client(auth=notion_token)
            
            # Format content for Notion
            blocks = self._convert_to_notion_blocks(report_content)
            
            # Create the page
            new_page = notion.pages.create(
                parent={"database_id": notion_database_id},
                properties={
                    "title": {
                        "title": [{"text": {"content": self.title}}]
                    },
                    "Tags": {
                        "multi_select": [{"name": tag} for tag in self.tags]
                    }
                },
                children=blocks
            )
            
            # Get the page URL
            page_id = new_page["id"]
            url = f"https://notion.so/{page_id.replace('-', '')}"
            
            return f"""Report generated successfully!
Title: {self.title}
Type: {self.report_type}
View in Notion: {url}"""
            
        except Exception as e:
            logger.error(f"Error saving to Notion: {str(e)}", exc_info=True)
            return f"Error saving to Notion: {str(e)}"
    
    def _convert_to_notion_blocks(self, report_content):
        """
        Converts report content to Notion blocks format.
        """
        blocks = []
        
        # Add header
        blocks.append({
            "object": "block",
            "type": "heading_1",
            "heading_1": {
                "rich_text": [{"type": "text", "text": {"content": report_content["title"]}}]
            }
        })
        
        # Add metadata
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [{"type": "text", "text": {"content": f"Generated: {report_content['generated_at']}"}}]
            }
        })
        
        # Add content sections
        for section, data in report_content["content"].items():
            # Add section header
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": section.replace('_', ' ').title()}}]
                }
            })
            
            # Add section content
            blocks.extend(self._format_section_content(data))
        
        return blocks
    
    def _format_section_content(self, data):
        """
        Formats section content into Notion blocks.
        """
        blocks = []
        
        if isinstance(data, dict):
            for key, value in data.items():
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": f"{key.replace('_', ' ').title()}: {value}"}}]
                    }
                })
        elif isinstance(data, list):
            for item in data:
                blocks.append({
                    "object": "block",
                    "type": "bulleted_list_item",
                    "bulleted_list_item": {
                        "rich_text": [{"type": "text", "text": {"content": str(item)}}]
                    }
                })
        else:
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": str(data)}}]
                }
            })
        
        return blocks
    
    def _format_as_text(self, report_content):
        """
        Formats the report content as plain text.
        """
        text = f"""
{report_content['title']}
{'=' * len(report_content['title'])}
Generated: {report_content['generated_at']}
Type: {report_content['type']}

"""
        
        for section, data in report_content["content"].items():
            text += f"\n{section.replace('_', ' ').title()}\n{'-' * len(section)}\n"
            if isinstance(data, dict):
                for key, value in data.items():
                    text += f"{key.replace('_', ' ').title()}: {value}\n"
            elif isinstance(data, list):
                for item in data:
                    text += f"- {item}\n"
            else:
                text += f"{data}\n"
        
        return text

if __name__ == "__main__":
    # Test the tool
    test_data = {
        "total_sales": {"value": 150000, "trend": "increasing"},
        "average_order": {"value": 75.50, "highlight": "15% increase from last month"},
        "customer_satisfaction": {"value": 4.2, "recommendation": "Focus on improving response time"},
        "top_products": ["Product A", "Product B", "Product C"],
        "data_sources": ["Sales Database", "CRM System"],
        "time_period": "Last 30 days",
        "analysis_methods": ["Trend Analysis", "Comparative Analysis"]
    }
    
    tool = ReportGenerator(
        report_type="executive_summary",
        title="Monthly Sales Performance Report",
        data=test_data,
        tags=["sales", "monthly", "performance"]
    )
    print(tool.run())

================
File: retail_agency/reporting_manager/tools/SQLQueryTool.py
================
import re
import os
import logging
import pandas as pd
import json
from datetime import datetime
from typing import Any, Optional, List, Dict, Tuple
from ast import literal_eval

from pydantic import Field, ConfigDict
from dotenv import load_dotenv

from langchain_community.utilities import SQLDatabase
from langchain_openai import ChatOpenAI
from agency_swarm.tools import BaseTool
from retail_agency.reporting_manager.tools.utils.CustomSQLTool import create_structured_sql_agent

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SQLQueryTool(BaseTool):
    """Tool for executing SQL queries with improved result handling."""
    
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    query: str = Field(
        ...,
        description="The natural language query to be converted into SQL and executed"
    )
    
    chunk_size: int = Field(
        default=100,
        description="Number of rows per chunk"
    )
    
    max_tokens: int = Field(
        default=4000,
        description="Maximum tokens for GPT response"
    )
    
    save_path: str = Field(
        default="inventory_results",
        description="Path to save query results"
    )
    
    _db: Optional[SQLDatabase] = None
    _agent_executor: Optional[Any] = None

    def __init__(self, **data):
        super().__init__(**data)
        os.makedirs(self.save_path, exist_ok=True)
        self._initialize_agent()

    def _initialize_agent(self):
        """Initialize SQL agent with database connection."""
        try:
            # Get database connection parameters
            db_host = os.getenv('CLOUD_DB_HOST')
            db_port = os.getenv('CLOUD_DB_PORT')
            db_user = os.getenv('CLOUD_DB_USER')
            db_pass = os.getenv('CLOUD_DB_PASS')
            db_name = os.getenv('CLOUD_DB_NAME')
            db_schema = os.getenv('CLOUD_SCHEMA')
            
            if not all([db_host, db_port, db_user, db_pass, db_name]):
                raise ValueError("Missing required database environment variables")
            
            database_url = f"postgresql+psycopg2://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}"
            logger.info(f"Connecting to database at {db_host}")
            
            self._db = SQLDatabase.from_uri(
                database_url,
                schema=db_schema,
                sample_rows_in_table_info=5
            )
            
            llm = ChatOpenAI(
                temperature=0,
                model="gpt-4",
                max_tokens=self.max_tokens
            )
            
            self._agent_executor = create_structured_sql_agent(
                llm=llm,
                db=self._db,
                verbose=True
            )
            
            logger.info("Successfully initialized SQL agent")
            
        except Exception as e:
            logger.error(f"Error initializing SQL agent: {str(e)}")
            raise

    def _parse_tuple_string(self, tuple_string: str) -> List[Tuple]:
        """Parse a string representation of a list of tuples into actual tuples."""
        try:
            # Using literal_eval to safely evaluate the string
            return literal_eval(tuple_string)
        except Exception as e:
            logger.error(f"Error parsing tuple string: {str(e)}")
            return []

    def _extract_sql_and_results(self, response: Dict[str, Any]) -> Tuple[str, List[Tuple]]:
        """Extract SQL query and results from agent response."""
        try:
            if not isinstance(response, dict) or 'output' not in response:
                raise ValueError("Invalid response format")

            # Try to parse the JSON output
            if isinstance(response['output'], str):
                parsed_output = json.loads(response['output'])
            else:
                parsed_output = response['output']

            sql_query = parsed_output.get('sql_query', '')
            raw_results = parsed_output.get('sql_result', [])

            # If results are a string representation of a list of tuples
            if isinstance(raw_results, str):
                results = self._parse_tuple_string(raw_results)
            else:
                results = raw_results

            return sql_query, results

        except json.JSONDecodeError:
            # Fallback to regex extraction if JSON parsing fails
            output = str(response.get('output', ''))
            sql_match = re.search(r'"sql_query":\s*"([^"]+)"', output)
            results_match = re.search(r'"sql_result":\s*(\[.*?\])', output, re.DOTALL)
            
            sql_query = sql_match.group(1) if sql_match else ''
            results = self._parse_tuple_string(results_match.group(1)) if results_match else []
            
            return sql_query, results

        except Exception as e:
            logger.error(f"Error extracting SQL and results: {str(e)}")
            return '', []

    def _convert_to_dataframe(self, results: Any) -> pd.DataFrame:
        """Convert SQL query results to DataFrame."""
        try:
            # Handle empty results
            if not results:
                return pd.DataFrame()

            # Handle list of tuples format
            if isinstance(results, list) and all(isinstance(x, tuple) for x in results):
                # Extract column names from the first tuple
                columns = ['item_id', 'name', 'stock_count']  # Hardcoded for now based on our query
                df = pd.DataFrame(results, columns=columns)
                logger.info(f"Created DataFrame with shape: {df.shape}")
                return df

            # Handle string format
            if isinstance(results, str):
                # Try to evaluate string as literal Python expression
                try:
                    data = literal_eval(results)
                    if isinstance(data, list) and data and isinstance(data[0], tuple):
                        columns = ['item_id', 'name', 'stock_count']
                        df = pd.DataFrame(data, columns=columns)
                        logger.info(f"Created DataFrame from string with shape: {df.shape}")
                        return df
                except:
                    pass

            # If all else fails, try to create DataFrame directly
            df = pd.DataFrame(results)
            logger.info(f"Created DataFrame from raw data with shape: {df.shape}")
            return df

        except Exception as e:
            logger.error(f"Error converting to DataFrame: {str(e)}")
            return pd.DataFrame()

    def _save_results(self, df: pd.DataFrame, base_name: str) -> Dict[str, str]:
        """Save results to both CSV and JSON files."""
        if df.empty:
            logger.warning("No data to save")
            return {}

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        paths = {}

        try:
            # Save CSV
            csv_filename = f"{base_name}_{timestamp}.csv"
            csv_path = os.path.join(self.save_path, csv_filename)
            df.to_csv(csv_path, index=False)
            paths['csv'] = csv_path
            logger.info(f"Saved results to CSV: {csv_path}")

            # Save JSON
            json_filename = f"{base_name}_{timestamp}.json"
            json_path = os.path.join(self.save_path, json_filename)
            df.to_json(json_path, orient='records', indent=2)
            paths['json'] = json_path
            logger.info(f"Saved results to JSON: {json_path}")

            return paths

        except Exception as e:
            logger.error(f"Error saving results: {str(e)}")
            return paths

    def run(self) -> dict:
        """Execute query and return structured results."""
        try:
            if not self._agent_executor:
                raise ValueError("SQL agent not properly initialized")
            
            # Get the response from the agent
            agent_response = self._agent_executor.invoke({"input": self.query})
            logger.info("Received response from agent")
            
            # Parse the JSON response
            if isinstance(agent_response, dict) and 'output' in agent_response:
                try:
                    response_data = json.loads(agent_response['output'])
                    sql_query = response_data.get('sql_query', '')
                    raw_results = response_data.get('sql_result', [])
                except json.JSONDecodeError:
                    # Fallback to regex if JSON parsing fails
                    output = str(agent_response['output'])
                    sql_match = re.search(r'"sql_query":\s*"([^"]+)"', output)
                    sql_query = sql_match.group(1) if sql_match else ''
                    results_match = re.search(r'"sql_result":\s*(\[.*?\])', output, re.DOTALL)
                    raw_results = results_match.group(1) if results_match else '[]'
            else:
                raise ValueError("Invalid response format from agent")

            if not sql_query:
                raise ValueError("Failed to extract SQL query from response")
            
            # Remove LIMIT clause for complete results
            base_query = re.sub(r"\s+LIMIT\s+\d+", "", sql_query, flags=re.IGNORECASE).strip()
            logger.info(f"Executing query: {base_query}")
            
            # Execute the full query
            db_results = self._db.run(base_query)
            
            # Convert results to DataFrame
            df = self._convert_to_dataframe(db_results)
            
            # Save results to both CSV and JSON
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            base_path = os.path.join(self.save_path, f"query_results_{timestamp}")
            
            # Save as CSV
            csv_path = f"{base_path}.csv"
            df.to_csv(csv_path, index=False)
            logger.info(f"Saved results to CSV: {csv_path}")
            
            # Save as JSON
            json_path = f"{base_path}.json"
            df.to_json(json_path, orient='records', indent=2)
            logger.info(f"Saved results to JSON: {json_path}")
            
            return {
                "type": "tabular",
                "data": df.to_dict(orient='records') if not df.empty else [],
                "row_count": len(df),
                "columns": df.columns.tolist() if not df.empty else [],
                "csv_path": csv_path,
                "json_path": json_path,
                "query": self.query,
                "sql_query": base_query
            }
            
        except Exception as e:
            error_msg = f"Error executing SQL query: {str(e)}"
            logger.error(error_msg)
            return {"type": "error", "error": error_msg}

    async def arun(self, *args, **kwargs) -> dict:
        """Asynchronous version of run."""
        return self.run()

================
File: retail_agency/reporting_manager/instructions.md
================
# Agent Role

I am the ReportingManager, an advanced analytics and reporting specialist within the Retail Management Agency. My role is to analyze retail data, generate insights, and create comprehensive reports that help drive business decisions. I use sophisticated statistical and machine learning methods to uncover patterns, predict trends, and identify opportunities for improvement. I can also execute SQL queries to retrieve and analyze data directly from the database.

# Goals

1. Provide accurate and actionable insights from retail data analysis
2. Execute SQL queries to retrieve data from the retail database
3. Generate comprehensive reports with clear visualizations and explanations
4. Identify trends, patterns, and anomalies in retail performance
5. Forecast future performance and provide strategic recommendations
6. Segment customers and analyze their behavior patterns
7. Monitor KPIs and alert stakeholders to significant changes
8. Ensure all reports are properly stored and accessible in Notion

# Process Workflow

1. Data Retrieval and Analysis
   - Execute SQL queries to retrieve relevant data
   - Validate and clean retrieved data
   - Perform appropriate analysis based on requirements (trend, forecast, segment, anomaly, correlation)
   - Apply statistical and machine learning methods to extract insights
   - Identify significant patterns and outliers

2. Report Generation
   - Create structured reports with clear sections and hierarchy
   - Include executive summaries for high-level insights
   - Provide detailed analysis with supporting data
   - Generate visualizations to illustrate key findings
   - Save reports to Notion with appropriate tags and organization

3. Insight Communication
   - Present findings in clear, business-friendly language
   - Highlight actionable recommendations
   - Provide context for technical analysis
   - Link insights to business objectives

4. Types of Analysis

   a. Trend Analysis
      - Analyze overall trends in key metrics
      - Identify seasonal patterns
      - Calculate growth rates
      - Provide summary statistics

   b. Forecasting
      - Generate time series forecasts
      - Provide confidence intervals
      - Evaluate forecast accuracy
      - Explain key drivers of predictions

   c. Customer Segmentation
      - Cluster customers based on behavior
      - Analyze segment characteristics
      - Track segment evolution
      - Recommend targeted strategies

   d. Anomaly Detection
      - Identify unusual patterns
      - Calculate threshold violations
      - Investigate root causes
      - Suggest preventive measures

   e. Correlation Analysis
      - Analyze relationships between metrics
      - Identify strong correlations
      - Explain causation vs correlation
      - Suggest areas for deeper analysis

5. Report Types

   a. Executive Summary
      - High-level overview
      - Key metrics and KPIs
      - Major findings and insights
      - Strategic recommendations

   b. Detailed Analysis
      - Comprehensive data breakdown
      - Statistical analysis results
      - Supporting evidence
      - Technical explanations

   c. Performance Dashboard
      - Real-time KPI tracking
      - Trend visualizations
      - Alert notifications
      - Comparative analysis

6. Follow-up Actions
   - Monitor implementation of recommendations
   - Track changes in key metrics
   - Update analysis as new data becomes available
   - Refine analytical models based on feedback

# Best Practices

1. Data Quality
   - Validate data before analysis
   - Handle missing values appropriately
   - Document data limitations
   - Note any assumptions made

2. Analysis Rigor
   - Use appropriate statistical methods
   - Test assumptions
   - Validate results
   - Document methodology

3. Report Clarity
   - Use clear, concise language
   - Provide context for findings
   - Include relevant visualizations
   - Structure information logically

4. Business Focus
   - Link analysis to business objectives
   - Provide actionable insights
   - Consider implementation feasibility
   - Prioritize recommendations

5. Documentation
   - Maintain detailed analysis logs
   - Document methodologies used
   - Track report versions
   - Store supporting data

# Technical Guidelines

1. **Database Interaction**
   - Use optimized SQL queries
   - Implement proper indexing
   - Handle large datasets efficiently
   - Maintain data integrity

2. **Report Formatting**
   - Use consistent formatting
   - Include clear headers and sections
   - Add explanatory notes where needed
   - Format numbers and dates appropriately
   - Use proper decimal places for currency

3. **Data Security**
   - Follow data privacy protocols
   - Mask sensitive information
   - Implement proper access controls
   - Maintain audit trails

4. **Performance Optimization**
   - Cache frequent queries
   - Use appropriate indexing
   - Implement query timeouts
   - Monitor query performance

# SQL Query Guidelines

1. Query Best Practices
   - Write clear, efficient SQL queries
   - Use appropriate joins to combine related data
   - Include proper filtering and aggregation
   - Format currency values with $ and 2 decimal places
   - Use clear column aliases for readability

2. Common Query Types
   - Sales analysis by product, category, or time period
   - Inventory levels and movement
   - Customer purchase patterns
   - Vendor performance metrics
   - Store performance comparisons

3. Performance Considerations
   - Use indexes effectively
   - Avoid SELECT *
   - Limit result sets when appropriate
   - Use efficient join conditions
   - Consider query execution time

4. Data Security
   - Respect data access permissions
   - Handle sensitive information appropriately
   - Follow data privacy guidelines
   - Log query execution for audit purposes

# Response Guidelines

1. **Standard Reports**
   - Include timestamp of data
   - Specify data range covered
   - Note any data limitations
   - Provide comparison periods

2. **Custom Analysis**
   - Document assumptions made
   - Explain methodology used
   - Highlight limitations
   - Suggest follow-up analysis

3. **Error Handling**
   - Report data inconsistencies
   - Document missing data
   - Provide alternative approaches
   - Suggest data quality improvements

4. **Follow-up Support**
   - Be available for clarifications
   - Provide additional analysis if needed
   - Document frequently requested reports
   - Maintain report templates

Remember: Your role is crucial in providing data-driven insights that enable informed business decisions. Always ensure accuracy, clarity, and actionability in your reports.

================
File: retail_agency/reporting_manager/reporting_manager.py
================
from agency_swarm import Agent
from tools.SQLQueryTool import SQLQueryTool
import logging
import json
from typing import Dict, Any, Optional
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ReportingManager(Agent):
    """Advanced reporting manager that handles database queries and analytics."""
    
    MAX_DISPLAY_ROWS = 10  # Maximum number of rows to display in natural language response
    
    def __init__(self):
        super().__init__(
            name="ReportingManager",
            description="Handles database queries and provides data-driven insights",
            instructions="./instructions.md",
            tools=[SQLQueryTool],
            temperature=0,
            model="gpt-4"
        )

    def _format_result_summary(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Format query results into a summary with sample data and file locations."""
        try:
            total_rows = result.get('row_count', 0)
            data = result.get('data', [])
            files = {
                "csv": result.get('csv_path'),
                "json": result.get('json_path')
            }
            
            summary = {
                "natural_response": "",
                "sample_data": [],
                "total_rows": total_rows,
                "files": files,
                "query": result.get('query'),
                "execution_time": datetime.now().isoformat()
            }
            
            # Generate natural language response based on result size
            if total_rows == 0:
                summary["natural_response"] = "No results found for your query."
            else:
                # Take first MAX_DISPLAY_ROWS for sample
                sample_data = data[:self.MAX_DISPLAY_ROWS]
                summary["sample_data"] = sample_data
                
                # Construct response based on data size
                if total_rows <= self.MAX_DISPLAY_ROWS:
                    summary["natural_response"] = f"Found {total_rows} items. Here are all the results:"
                else:
                    # Include file information in the response
                    summary["natural_response"] = (
                        f"Found {total_rows} items. Here are the first {self.MAX_DISPLAY_ROWS} items.\n"
                        f"Complete results have been saved to:\n"
                    )
                    
                    if files["csv"]:
                        summary["natural_response"] += f"- CSV file: {files['csv']}\n"
                    if files["json"]:
                        summary["natural_response"] += f"- JSON file: {files['json']}\n"
                    
                    summary["natural_response"] += "\nYou can access the full dataset using these files."
            
            # Add metadata about the query execution
            summary["metadata"] = {
                "execution_time": summary["execution_time"],
                "total_rows": total_rows,
                "displayed_rows": len(summary["sample_data"]),
                "has_more": total_rows > self.MAX_DISPLAY_ROWS,
                "files_saved": bool(files["csv"] or files["json"])
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"Error formatting result summary: {str(e)}")
            return {
                "natural_response": f"Error formatting results: {str(e)}",
                "sample_data": [],
                "total_rows": 0,
                "files": {},
                "query": result.get('query'),
                "execution_time": datetime.now().isoformat()
            }

    def _validate_query_result(self, result: Dict[str, Any]) -> bool:
        """Validate that the query result has the expected structure."""
        required_fields = ['type', 'data', 'row_count']
        return all(field in result for field in required_fields)

    def handle_message(self, message: str) -> Dict[str, Any]:
        """Handle incoming messages and return structured responses."""
        try:
            logger.info(f"Processing query: {message}")
            
            # Execute query with proper initialization check
            tool = SQLQueryTool(query=message)
            
            if not hasattr(tool, '_agent_executor') or tool._agent_executor is None:
                raise ValueError("SQL agent failed to initialize. Please check database connection parameters.")
                
            result = tool.run()
            logger.info(f"Received query result: {result}")
            
            # Validate result
            if not self._validate_query_result(result):
                if isinstance(result, dict) and 'error' in result:
                    raise ValueError(f"Query execution failed: {result['error']}")
                raise ValueError("Invalid query result structure")
            
            # Format the response
            summary = self._format_result_summary(result)
            
            logger.info(f"Generated response summary with {summary['total_rows']} total rows")
            return summary
            
        except ValueError as ve:
            error_msg = str(ve)
            logger.error(error_msg)
            return {
                "natural_response": f"Error: {error_msg}",
                "sample_data": [],
                "total_rows": 0,
                "files": {},
                "query": message,
                "execution_time": datetime.now().isoformat(),
                "error": error_msg
            }
        except Exception as e:
            error_msg = f"Error processing message: {str(e)}"
            logger.error(error_msg)
            return {
                "natural_response": f"Error: {error_msg}",
                "sample_data": [],
                "total_rows": 0,
                "files": {},
                "query": message,
                "execution_time": datetime.now().isoformat(),
                "error": error_msg
            }

    def __str__(self) -> str:
        return f"ReportingManager(max_display_rows={self.MAX_DISPLAY_ROWS})"

if __name__ == "__main__":
    # Test the ReportingManager
    manager = ReportingManager()
    test_query = "Get me all inventory items with stock count less than 0"
    
    try:
        result = manager.handle_message(test_query)
        
        print("\nQuery Response:")
        print(result["natural_response"])
        
        if result["sample_data"]:
            print("\nSample Data:")
            for idx, item in enumerate(result["sample_data"], 1):
                print(f"\nItem {idx}:")
                for key, value in item.items():
                    print(f"{key}: {value}")
                    
            if result["total_rows"] > len(result["sample_data"]):
                print("\nFull results saved to:")
                if result["files"].get("csv"):
                    print(f"CSV: {result['files']['csv']}")
                if result["files"].get("json"):
                    print(f"JSON: {result['files']['json']}")
        
        # Print metadata if available
        if "metadata" in result:
            print("\nQuery Metadata:")
            for key, value in result["metadata"].items():
                print(f"{key}: {value}")
            
    except Exception as e:
        print(f"Error in test execution: {str(e)}")

================
File: retail_agency/utils/db_connection.py
================
"""Database connection utilities for Cloud SQL PostgreSQL database.

This module provides utilities for connecting to a Cloud SQL PostgreSQL database
using either the Cloud SQL Python Connector or direct connection via pg8000.
It includes connection pooling, retry logic, and comprehensive error handling.
"""

import os
import logging
import time
from contextlib import contextmanager
from typing import Any, Generator, Optional, Dict, Callable
from dataclasses import dataclass
from pathlib import Path

import pg8000
from google.cloud.sql.connector import Connector, IPTypes
from google.oauth2 import service_account
from sqlalchemy import create_engine, URL, Engine, text
from sqlalchemy.pool import QueuePool
from tenacity import retry, stop_after_attempt, wait_exponential

# Configure logging with more detail
logging.basicConfig(
    level=logging.DEBUG,  # Set to DEBUG for more detailed logs
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Configure logging
logger = logging.getLogger(__name__)

# Database Configuration Constants
DB_CONFIG = {
    "INSTANCE_CONNECTION_NAME": "perfect-rider-446204-h0:us-central1:prod-7rivermart",
    "DB_USER": "postgres",
    "DB_PASS": "Jz7c[[AMBi9j5yS)",
    "DB_NAME": "postgres",
    "DB_HOST": "34.57.51.199",
    "CREDENTIALS_PATH": str(Path(__file__).parent.parent / "cloudsql-credentials.json"),
    "MAX_CONNECTIONS": 10,
    "POOL_TIMEOUT": 30,
    "RETRY_ATTEMPTS": 3,
    "RETRY_MIN_WAIT": 1,  # seconds
    "RETRY_MAX_WAIT": 10  # seconds
}

@dataclass
class DatabaseConfig:
    """Database configuration container."""
    instance_connection_name: str
    user: str
    password: str
    database: str
    host: str
    credentials_path: str
    max_connections: int = 10
    pool_timeout: int = 30

    @classmethod
    def from_env(cls) -> 'DatabaseConfig':
        """Create configuration from environment variables."""
        return cls(
            instance_connection_name=os.getenv('CLOUD_SQL_INSTANCE', DB_CONFIG['INSTANCE_CONNECTION_NAME']),
            user=os.getenv('CLOUD_DB_USER', DB_CONFIG['DB_USER']),
            password=os.getenv('CLOUD_DB_PASS', DB_CONFIG['DB_PASS']),
            database=os.getenv('CLOUD_DB_NAME', DB_CONFIG['DB_NAME']),
            host=os.getenv('CLOUD_DB_HOST', DB_CONFIG['DB_HOST']),
            credentials_path=os.getenv('CLOUD_DB_CREDENTIALS_PATH', DB_CONFIG['CREDENTIALS_PATH']),
            max_connections=int(os.getenv('DB_MAX_CONNECTIONS', DB_CONFIG['MAX_CONNECTIONS'])),
            pool_timeout=int(os.getenv('DB_POOL_TIMEOUT', DB_CONFIG['POOL_TIMEOUT']))
        )

class DatabaseError(Exception):
    """Base class for database-related exceptions."""
    pass

class CredentialsError(DatabaseError):
    """Raised when there are issues with database credentials."""
    pass

class ConnectionError(DatabaseError):
    """Raised when database connection fails."""
    pass

def get_credentials(credentials_path: str = None) -> service_account.Credentials:
    """Get Google Cloud credentials from service account file."""
    try:
        if not credentials_path:
            credentials_path = str(Path(__file__).parent.parent / "cloudsql-credentials.json")
            logger.info(f"Using default credentials path: {credentials_path}")
            
        if not os.path.exists(credentials_path):
            logger.error(f"Credentials file not found at {credentials_path}")
            raise CredentialsError(f"Credentials file not found at {credentials_path}")
            
        logger.info(f"Loading credentials from: {credentials_path}")
        credentials = service_account.Credentials.from_service_account_file(
            credentials_path,
            scopes=["https://www.googleapis.com/auth/sqlservice.admin"]
        )
        logger.info(f"Successfully loaded credentials for service account: {credentials.service_account_email}")
        return credentials
    except Exception as e:
        logger.error(f"Failed to load credentials from {credentials_path}: {str(e)}", exc_info=True)
        raise CredentialsError(f"Failed to load credentials: {str(e)}")

@retry(
    stop=stop_after_attempt(DB_CONFIG['RETRY_ATTEMPTS']),
    wait=wait_exponential(
        multiplier=DB_CONFIG['RETRY_MIN_WAIT'],
        max=DB_CONFIG['RETRY_MAX_WAIT']
    ),
    reraise=True
)
def create_connector_connection(config: DatabaseConfig) -> Any:
    """
    Create a database connection using Cloud SQL Connector.
    
    Args:
        config: Database configuration object.
        
    Returns:
        Connection object.
        
    Raises:
        ConnectionError: If connection cannot be established.
    """
    try:
        logger.info(f"Creating Cloud SQL connection for instance: {config.instance_connection_name}")
        
        # Get service account credentials
        credentials = get_credentials(config.credentials_path)
        logger.info(f"Using service account: {credentials.service_account_email}")
        
        # Initialize connector with credentials
        connector = Connector(credentials=credentials)
        
        # Create connection
        conn = connector.connect(
            instance_connection_string=config.instance_connection_name,
            driver="pg8000",
            db=config.database,
            user=config.user,
            password=config.password,
            enable_iam_auth=True,
            ip_type=IPTypes.PUBLIC  # Use public IP
        )
        
        logger.info("Cloud SQL Connector connection successful!")
        return conn
        
    except Exception as e:
        logger.error(f"Failed to create connector connection: {str(e)}", exc_info=True)
        raise ConnectionError(f"Failed to create connector connection: {str(e)}") from e

@retry(
    stop=stop_after_attempt(DB_CONFIG['RETRY_ATTEMPTS']),
    wait=wait_exponential(
        multiplier=DB_CONFIG['RETRY_MIN_WAIT'],
        max=DB_CONFIG['RETRY_MAX_WAIT']
    ),
    reraise=True
)
def create_direct_connection(config: DatabaseConfig) -> Any:
    """Create a direct connection to the database using pg8000."""
    try:
        logger.info(f"Attempting direct connection to {config.host}:{config.database}")
        logger.debug(f"Connection details - Host: {config.host}, DB: {config.database}, User: {config.user}")
        
        # Get credentials
        credentials = get_credentials(config.credentials_path)
        logger.info(f"Using service account: {credentials.service_account_email}")
        
        # Create connection without SSL
        conn = pg8000.connect(
            database=config.database,
            user=config.user,
            password=config.password,
            host=config.host,
            ssl_context=False  # Disable SSL since it's not configured
        )
        
        logger.info("Direct connection successful!")
        return conn
        
    except Exception as e:
        logger.error(f"Failed to create direct connection to {config.host}: {str(e)}")
        raise ConnectionError(f"Failed to create direct connection: {str(e)}") from e

def get_connection_factory(config: DatabaseConfig) -> Callable[[], Any]:
    """
    Create a connection factory function.
    
    Args:
        config: Database configuration object.
        
    Returns:
        Callable that creates database connections.
    """
    def get_conn() -> Any:
        try:
            # Check if running on Cloud Run
            is_cloud_run = os.getenv('K_SERVICE') is not None
            
            if is_cloud_run:
                logger.info("Running on Cloud Run, using Cloud SQL Connector")
                return create_connector_connection(config)
            
            # Local development - try connector first, fall back to direct
            try:
                return create_connector_connection(config)
            except Exception as e:
                logger.warning(f"Cloud SQL Connector connection failed: {str(e)}")
                logger.info("Falling back to direct connection...")
                return create_direct_connection(config)
                
        except Exception as e:
            logger.error(f"Connection failed: {str(e)}")
            raise
    
    return get_conn

def get_db_engine(config: Optional[DatabaseConfig] = None) -> Engine:
    """
    Create and configure a SQLAlchemy engine with connection pooling.
    
    Args:
        config: Optional database configuration object. If not provided,
               configuration will be loaded from environment.
               
    Returns:
        SQLAlchemy Engine instance.
        
    Raises:
        DatabaseError: If engine creation fails.
    """
    try:
        if config is None:
            config = DatabaseConfig.from_env()
        
        # Create the connection factory
        creator = get_connection_factory(config)
        
        # Create SQLAlchemy engine with connection pooling
        engine = create_engine(
            "postgresql+pg8000://",
            creator=creator,
            poolclass=QueuePool,
            pool_size=config.max_connections,
            max_overflow=2,
            pool_timeout=config.pool_timeout,
            pool_pre_ping=True
        )
        
        # Test the connection
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        logger.info("SQLAlchemy engine created successfully")
        return engine
        
    except Exception as e:
        raise DatabaseError(f"Failed to create database engine: {str(e)}") from e

@contextmanager
def get_db_connection(config: Optional[DatabaseConfig] = None) -> Generator[Any, None, None]:
    """
    Context manager for database connections.
    
    Args:
        config: Optional database configuration object.
        
    Yields:
        Database connection object.
    """
    if config is None:
        config = DatabaseConfig.from_env()
    
    conn = None
    try:
        creator = get_connection_factory(config)
        conn = creator()
        yield conn
    finally:
        if conn:
            try:
                conn.close()
            except Exception as e:
                logger.warning(f"Error closing connection: {str(e)}")

@contextmanager
def get_db_cursor(config: Optional[DatabaseConfig] = None) -> Generator[Any, None, None]:
    """
    Context manager for database cursors.
    
    Args:
        config: Optional database configuration object.
        
    Yields:
        Database cursor object.
    """
    with get_db_connection(config) as conn:
        cursor = conn.cursor()
        try:
            yield cursor
            conn.commit()
        finally:
            cursor.close()

def test_connection(config: Optional[DatabaseConfig] = None) -> bool:
    """
    Test database connection and schema access.
    
    Args:
        config: Optional database configuration object.
        
    Returns:
        bool: True if connection test passes, False otherwise.
    """
    try:
        with get_db_cursor(config) as cursor:
            # Test schema query
            cursor.execute("""
                SELECT schema_name 
                FROM information_schema.schemata 
                WHERE schema_name NOT IN ('information_schema', 'pg_catalog')
            """)
            schemas = cursor.fetchall()
            
            logger.info("Available schemas:")
            for schema in schemas:
                logger.info(f"- {schema[0]}")
                
                # Test table access in each schema
                cursor.execute("""
                    SELECT table_name 
                    FROM information_schema.tables 
                    WHERE table_schema = %s
                """, (schema[0],))
                tables = cursor.fetchall()
                
                if tables:
                    logger.info(f"\nTables in {schema[0]} schema:")
                    for table in tables:
                        logger.info(f"- {schema[0]}.{table[0]}")
            
            logger.info("Connection test successful!")
            return True
            
    except Exception as e:
        logger.error(f"Connection test failed: {str(e)}")
        return False

if __name__ == "__main__":
    # Set up logging for the test
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info("Starting database connection tests...")
    try:
        # Test the connection
        if test_connection():
            logger.info("All database connection tests passed!")
        else:
            logger.error("Database connection tests failed!")
    except Exception as e:
        logger.error(f"Error during testing: {str(e)}", exc_info=True)

================
File: retail_agency/utils/firebase_db.py
================
from pathlib import Path
from firebase_admin import initialize_app, credentials, firestore
import logging
from datetime import datetime
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)

# Initialize Firebase
service_account_key = Path(__file__).parent.parent / "firebase-credentials.json"
try:
    client_credentials = credentials.Certificate(str(service_account_key))
    initialize_app(client_credentials)
    db = firestore.client()
    logger.info("Firebase initialized successfully")
except Exception as e:
    logger.error(f"Failed to initialize Firebase: {str(e)}")
    raise

def get_threads_from_db(conversation_id: str) -> dict:
    """
    Retrieve threads for a specific conversation from Firestore.
    
    Args:
        conversation_id: Unique identifier for the conversation (channel_id:thread_ts)
        
    Returns:
        Dict containing thread data or empty dict if not found
    """
    try:
        doc = db.collection('slack-chats').document(conversation_id).get()
        
        if doc.exists:
            return doc.to_dict()['threads']
        return {}
        
    except Exception as e:
        logger.error(f"Error retrieving threads from Firestore: {str(e)}")
        return {}

def save_threads_to_db(conversation_id: str, threads: dict) -> None:
    """
    Save threads for a specific conversation to Firestore.
    
    Args:
        conversation_id: Unique identifier for the conversation (channel_id:thread_ts)
        threads: Dictionary containing thread data
    """
    try:
        db.collection('slack-chats').document(conversation_id).set({
            'threads': threads,
            'updated_at': datetime.utcnow()
        }, merge=True)
        
        logger.info(f"Saved threads for conversation {conversation_id}")
        
    except Exception as e:
        logger.error(f"Error saving threads to Firestore: {str(e)}")
        raise

class SlackThreadStorage:
    """Legacy class maintained for backward compatibility."""
    
    def __init__(self):
        self.db = db
        self.collection = self.db.collection('slack-chats')

    def save_thread(self, 
                   channel_id: str, 
                   thread_ts: str, 
                   user_id: str, 
                   initial_message: str,
                   metadata: Optional[Dict[str, Any]] = None) -> str:
        """Save a new Slack thread to Firestore."""
        try:
            conversation_id = f"{channel_id}:{thread_ts}"
            
            thread_data = {
                'channel_id': channel_id,
                'thread_ts': thread_ts,
                'user_id': user_id,
                'initial_message': initial_message,
                'created_at': datetime.utcnow(),
                'updated_at': datetime.utcnow(),
                'is_active': True,
                'message_count': 1,
                'metadata': metadata or {}
            }
            
            # Get existing threads
            existing_threads = get_threads_from_db(conversation_id)
            
            # Add new thread
            existing_threads[thread_ts] = thread_data
            
            # Save updated threads
            save_threads_to_db(conversation_id, existing_threads)
            
            return thread_ts
            
        except Exception as e:
            logger.error(f"Failed to save thread to Firestore: {str(e)}")
            raise

    def update_thread(self, 
                     thread_ts: str, 
                     updates: Dict[str, Any],
                     channel_id: str = None) -> None:
        """Update an existing thread record."""
        try:
            if not channel_id:
                raise ValueError("channel_id is required for update_thread")
                
            conversation_id = f"{channel_id}:{thread_ts}"
            
            # Get existing threads
            existing_threads = get_threads_from_db(conversation_id)
            
            if thread_ts not in existing_threads:
                raise ValueError(f"Thread {thread_ts} not found")
                
            # Update thread data
            existing_threads[thread_ts].update(updates)
            existing_threads[thread_ts]['updated_at'] = datetime.utcnow()
            
            # Save updated threads
            save_threads_to_db(conversation_id, existing_threads)
            
        except Exception as e:
            logger.error(f"Failed to update thread in Firestore: {str(e)}")
            raise

    def get_thread(self, thread_ts: str, channel_id: str) -> Optional[Dict[str, Any]]:
        """Retrieve a thread record by its timestamp."""
        try:
            conversation_id = f"{channel_id}:{thread_ts}"
            threads = get_threads_from_db(conversation_id)
            return threads.get(thread_ts)
            
        except Exception as e:
            logger.error(f"Failed to retrieve thread from Firestore: {str(e)}")
            raise

    def add_message_to_thread(self,
                            thread_ts: str,
                            message: str,
                            user_id: str,
                            message_ts: str,
                            channel_id: str) -> None:
        """Add a new message to an existing thread."""
        try:
            conversation_id = f"{channel_id}:{thread_ts}"
            
            # Get existing threads
            threads = get_threads_from_db(conversation_id)
            
            if thread_ts not in threads:
                raise ValueError(f"Thread {thread_ts} not found")
                
            # Add message to thread
            if 'messages' not in threads[thread_ts]:
                threads[thread_ts]['messages'] = {}
                
            threads[thread_ts]['messages'][message_ts] = {
                'content': message,
                'user_id': user_id,
                'timestamp': message_ts,
                'created_at': datetime.utcnow()
            }
            
            # Update thread metadata
            threads[thread_ts]['updated_at'] = datetime.utcnow()
            threads[thread_ts]['message_count'] = len(threads[thread_ts]['messages'])
            
            # Save updated threads
            save_threads_to_db(conversation_id, threads)
            
        except Exception as e:
            logger.error(f"Failed to add message to thread: {str(e)}")
            raise

    def get_thread_messages(self, 
                          thread_ts: str,
                          channel_id: str,
                          limit: int = 100) -> list:
        """Retrieve messages from a thread."""
        try:
            conversation_id = f"{channel_id}:{thread_ts}"
            threads = get_threads_from_db(conversation_id)
            
            if thread_ts not in threads or 'messages' not in threads[thread_ts]:
                return []
                
            messages = list(threads[thread_ts]['messages'].values())
            messages.sort(key=lambda x: x['timestamp'])
            
            return messages[:limit]
            
        except Exception as e:
            logger.error(f"Failed to retrieve thread messages: {str(e)}")
            raise

    def close_thread(self, thread_ts: str, channel_id: str) -> None:
        """Mark a thread as inactive/closed."""
        try:
            self.update_thread(
                thread_ts=thread_ts,
                channel_id=channel_id,
                updates={'is_active': False}
            )
            logger.info(f"Closed thread {thread_ts}")
            
        except Exception as e:
            logger.error(f"Failed to close thread: {str(e)}")
            raise

================
File: retail_agency/utils/slack_handler.py
================
from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler
import os
import logging
from typing import Optional, Dict, Any
from pathlib import Path
from agency_swarm import Agency
from ..agency import generate_response

logger = logging.getLogger(__name__)

class SlackEventHandler:
    def __init__(self):
        self.app = App(token=os.environ["SLACK_BOT_TOKEN"])
        self._setup_event_handlers()

    def _setup_event_handlers(self):
        """Set up all Slack event handlers."""
        
        @self.app.event("message")
        def handle_message(event, say):
            """Handle incoming messages."""
            try:
                # Ignore bot messages
                if event.get("bot_id"):
                    return

                channel_id = event["channel"]
                user_id = event["user"]
                message = event["text"]
                message_ts = event["ts"]
                thread_ts = event.get("thread_ts", message_ts)  # Use message_ts as thread_ts for new threads
                
                # Create conversation_id for Firebase
                conversation_id = f"{channel_id}:{thread_ts}"

                # Process the message using Agency
                response = generate_response(message, conversation_id)

                # Send response
                say(text=response, thread_ts=thread_ts)

            except Exception as e:
                logger.error(f"Error handling message event: {str(e)}", exc_info=True)
                # Notify user of error if appropriate
                say(text="Sorry, I encountered an error processing your message.", thread_ts=thread_ts)

    def start(self):
        """Start the Slack event handler."""
        try:
            handler = SocketModeHandler(
                app=self.app,
                app_token=os.environ["SLACK_APP_TOKEN"]
            )
            logger.info("Starting Slack event handler...")
            handler.start()
        except Exception as e:
            logger.error(f"Failed to start Slack event handler: {str(e)}")
            raise

if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Start the event handler
    handler = SlackEventHandler()
    handler.start()

================
File: retail_agency/agency_manifesto.md
================
# Retail Management Agency

## Agency Description
This agency provides comprehensive retail management services through a team of specialized AI agents. The agency handles various aspects of retail operations, from data analysis and reporting to store operations and customer service management.

## Mission Statement
To deliver efficient, data-driven retail management solutions by coordinating specialized AI agents that work together to provide accurate, timely, and actionable insights for retail businesses.

## Operating Environment
The agency operates in a retail management context, processing requests through Slack and interfacing with retail databases. The environment includes:

1. **Communication Platform**
   - Primary communication through Slack
   - Threaded conversations for organized communication
   - Real-time updates and notifications

2. **Data Infrastructure**
   - Retail database access
   - Business intelligence tools
   - Secure data handling protocols

3. **Operational Framework**
   - Clear delegation paths
   - Defined roles and responsibilities
   - Quality assurance processes
   - Performance monitoring systems

## Shared Guidelines

1. **Communication Standards**
   - Professional and clear communication
   - Prompt response times
   - Organized threading
   - Clear status updates

2. **Data Handling**
   - Secure data processing
   - Privacy compliance
   - Data accuracy verification
   - Documentation requirements

3. **Quality Standards**
   - Accuracy in all deliverables
   - Thorough review processes
   - Consistent formatting
   - Professional presentation

4. **Collaboration Protocol**
   - Clear task delegation
   - Efficient information sharing
   - Cross-functional coordination
   - Regular progress updates

================
File: retail_agency/agency.py
================
from dotenv import load_dotenv
from pathlib import Path
from agency_swarm import Agency, set_openai_key, set_openai_client
from ceo.ceo import CEO
from reporting_manager.reporting_manager import ReportingManager
import openai
import os

# Load environment variables
load_dotenv(Path(__file__).parent / ".env")

# Configure OpenAI
set_openai_key(os.environ["OPENAI_API_KEY"])

# Initialize OpenAI client with project API key configuration
client = openai.OpenAI(
    api_key=os.environ["OPENAI_API_KEY"],
    max_retries=10,
    default_headers={"OpenAI-Beta": "assistants=v2"}
)
set_openai_client(client)

# Initialize agents
ceo = CEO()
reporting_manager = ReportingManager()

# Create agency
agency = Agency(
    [
        ceo,  # CEO is the entry point
        [ceo, reporting_manager],  # CEO can communicate with Reporting Manager
    ],
    shared_instructions='./agency_manifesto.md'
)

if __name__ == "__main__":
    agency.run_demo()

================
File: retail_agency/app.py
================
from pathlib import Path
from agency_swarm import Agency, set_openai_key, set_openai_client
from ceo.ceo import CEO
from reporting_manager.reporting_manager import ReportingManager
from firebase_admin import initialize_app, credentials, firestore, get_app
from dotenv import load_dotenv
import openai
import os
import logging
from datetime import datetime
from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler
import time
from utils.firebase_db import get_threads_from_db, save_threads_to_db

# Configure logging
logging.basicConfig(
    filename='retail_agency_bot.log',
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv(Path(__file__).parent / ".env")

# Configure OpenAI
set_openai_key(os.environ["OPENAI_API_KEY"])
client = openai.OpenAI(
    api_key=os.environ["OPENAI_API_KEY"],
    max_retries=10,
    default_headers={"OpenAI-Beta": "assistants=v2"}
)
set_openai_client(client)

# Initialize Firebase
try:
    # Try to get existing app
    firebase_app = get_app()
    logger.info("Using existing Firebase app")
except ValueError:
    # Initialize new app if none exists
    service_account_key = Path(__file__).parent / "firebase-credentials.json"
    cred = credentials.Certificate(service_account_key)
    firebase_app = initialize_app(cred, name='retail-agency')
    logger.info("Initialized new Firebase app")

db = firestore.client()

# Initialize agents
ceo = CEO()
reporting_manager = ReportingManager()

# Create agency
agency = Agency(
    [
        ceo,  # CEO is the entry point
        [ceo, reporting_manager],  # CEO can communicate with Reporting Manager
    ],
    shared_instructions='./agency_manifesto.md'
)

def wait_for_run_completion(thread_id, run_id, max_wait_time=60):
    """Wait for an active run to complete."""
    start_time = time.time()
    while time.time() - start_time < max_wait_time:
        try:
            run = client.beta.threads.runs.retrieve(
                thread_id=thread_id,
                run_id=run_id
            )
            if run.status in ['completed', 'failed', 'cancelled']:
                return True
            time.sleep(2)
        except Exception as e:
            logger.error(f"Error checking run status: {str(e)}")
            return False
    return False

def generate_response(message: str, conversation_id: str) -> str:
    """Generate a response using the agency."""
    try:
        # Get completion from agency
        completion = agency.get_completion(message, yield_messages=False)
        return completion
    except Exception as e:
        logger.error(f"Error generating response: {str(e)}", exc_info=True)
        return "I apologize, but I encountered an error processing your request. Please try again in a moment."

# Initialize the Slack Bolt app
app = App(token=os.environ["SLACK_BOT_TOKEN"])

@app.message()
def handle_message(message, say):
    """Handle incoming messages."""
    try:
        # Ignore bot messages
        if message.get("bot_id"):
            return

        channel_id = message["channel"]
        user_id = message["user"]
        text = message["text"]
        message_ts = message["ts"]
        thread_ts = message.get("thread_ts", message_ts)  # Use message_ts as thread_ts for new threads
        
        # Create conversation_id for Firebase
        conversation_id = f"{channel_id}:{thread_ts}"
        
        logging.info(f"Processing message from user {user_id} in channel {channel_id}")
        
        if text:
            # Get response from agency
            response = generate_response(text, conversation_id)
            
            # Send response
            response_msg = say(text=response, thread_ts=thread_ts)
            
            # Get existing threads
            threads = get_threads_from_db(conversation_id)
            
            # Initialize threads dict if it doesn't exist
            if not threads:
                threads = {}
            
            # If this is a new thread or thread doesn't exist yet
            if thread_ts not in threads:
                threads[thread_ts] = {
                    'channel_id': channel_id,
                    'thread_ts': thread_ts,
                    'user_id': user_id,
                    'initial_message': text,
                    'messages': {},
                    'is_active': True,
                    'message_count': 0
                }
            
            # Ensure messages dict exists
            if 'messages' not in threads[thread_ts]:
                threads[thread_ts]['messages'] = {}
                
            threads[thread_ts]['messages'][message_ts] = {
                'content': text,
                'user_id': user_id,
                'timestamp': message_ts,
                'created_at': datetime.utcnow()
            }
            
            # Add bot response
            if response_msg:
                bot_msg_ts = response_msg['ts']
                threads[thread_ts]['messages'][bot_msg_ts] = {
                    'content': response,
                    'user_id': 'BOT',
                    'timestamp': bot_msg_ts,
                    'created_at': datetime.utcnow()
                }
            
            # Update thread metadata
            threads[thread_ts]['message_count'] = len(threads[thread_ts]['messages'])
            threads[thread_ts]['updated_at'] = datetime.utcnow()
            
            # Save updated threads
            save_threads_to_db(conversation_id, threads)
            
            logging.info("Message processed and saved successfully")

    except Exception as e:
        logger.error(f"Error processing message: {str(e)}", exc_info=True)
        # Notify user of error if appropriate
        say(text="Sorry, I encountered an error processing your message.", thread_ts=thread_ts)

def start_bot():
    """Initialize and start the Slack bot."""
    try:
        # Print startup message
        logger.info("=" * 50)
        logger.info("Starting Retail Management Agency Slack Bot...")
        logger.info(f"Bot started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 50)
        
        # Start the Socket Mode handler
        handler = SocketModeHandler(
            app=app,
            app_token=os.environ["SLACK_APP_TOKEN"]
        )
        handler.start()
        
    except Exception as e:
        logger.error(f"Error starting bot: {str(e)}")
        raise

if __name__ == "__main__":
    start_bot()

================
File: retail_agency/main.py
================
import os
import logging
from pathlib import Path
from dotenv import load_dotenv
from utils.slack_handler import SlackEventHandler

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    # Load environment variables
    load_dotenv(Path(__file__).parent / ".env")
    
    # Required environment variables
    required_env_vars = [
        "SLACK_BOT_TOKEN",
        "SLACK_APP_TOKEN",
        "OPENAI_API_KEY"
    ]
    
    # Check for required environment variables
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    if missing_vars:
        raise ValueError(f"Missing required environment variables: {', '.join(missing_vars)}")
    
    try:
        # Initialize and start Slack handler
        handler = SlackEventHandler()
        logger.info("Starting Slack bot...")
        handler.start()
        
    except Exception as e:
        logger.error(f"Failed to start application: {str(e)}", exc_info=True)
        raise

if __name__ == "__main__":
    main()

================
File: retail_agency/requirements.txt
================
agency-swarm>=0.1.0
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
statsmodels>=0.14.0
notion-client>=2.0.0
python-dotenv>=1.0.0
langchain>=0.1.0
langchain-community>=0.0.10
sqlalchemy>=2.0.0
psycopg2-binary>=2.9.0  # For PostgreSQL
openai>=1.12.0
slack-bolt>=1.18.0
slack-sdk>=3.21.3
pydantic>=2.8.2
pytz>=2024.1
cloud-sql-python-connector[pg8000]>=1.16.0
google-auth>=2.38.0
google-cloud-core>=2.0.0
langchain-openai>=0.0.5  # For OpenAI integration
pg8000>=1.30.5  # Python PostgreSQL driver
pathlib>=1.0.1
gradio>=4.19.2  # For the demo interface

================
File: retail_agency/retail_agency_threads.json
================
{"CEO": {"ReportingManager": "thread_lDAQ8yDMtUctUAmCqyIpZqIy"}, "main_thread": "thread_Y58RNyzRFz1RSFz3zCVsxLXC"}

================
File: retail_agency/settings.json
================
[
    {
        "id": "asst_WaJeRq8zdtG6rd9wjYyO1TVe",
        "created_at": 1738977721,
        "description": "Retail Management Agency CEO responsible for client communication, task delegation, and coordinating with specialized agents like the ReportingManager for data analysis and insights.",
        "instructions": "# Retail Management Agency\n\n## Agency Description\nThis agency provides comprehensive retail management services through a team of specialized AI agents. The agency handles various aspects of retail operations, from data analysis and reporting to store operations and customer service management.\n\n## Mission Statement\nTo deliver efficient, data-driven retail management solutions by coordinating specialized AI agents that work together to provide accurate, timely, and actionable insights for retail businesses.\n\n## Operating Environment\nThe agency operates in a retail management context, processing requests through Slack and interfacing with retail databases. The environment includes:\n\n1. **Communication Platform**\n   - Primary communication through Slack\n   - Threaded conversations for organized communication\n   - Real-time updates and notifications\n\n2. **Data Infrastructure**\n   - Retail database access\n   - Business intelligence tools\n   - Secure data handling protocols\n\n3. **Operational Framework**\n   - Clear delegation paths\n   - Defined roles and responsibilities\n   - Quality assurance processes\n   - Performance monitoring systems\n\n## Shared Guidelines\n\n1. **Communication Standards**\n   - Professional and clear communication\n   - Prompt response times\n   - Organized threading\n   - Clear status updates\n\n2. **Data Handling**\n   - Secure data processing\n   - Privacy compliance\n   - Data accuracy verification\n   - Documentation requirements\n\n3. **Quality Standards**\n   - Accuracy in all deliverables\n   - Thorough review processes\n   - Consistent formatting\n   - Professional presentation\n\n4. **Collaboration Protocol**\n   - Clear task delegation\n   - Efficient information sharing\n   - Cross-functional coordination\n   - Regular progress updates \n\n# Agent Role\n\nYou are the CEO of a retail management agency, serving as the primary point of contact for all user communications. Your role is to understand user requests, analyze their needs, and efficiently delegate tasks to the appropriate specialized agents while maintaining professional and helpful communication.\n\n# Goals\n\n1. **Request Analysis and Delegation**\n   - Analyze incoming requests to understand their requirements\n   - For data/analytics requests:\n     * Delegate to ReportingManager\n     * Wait for and process their response\n     * Post detailed results to Notion if:\n       - The data contains more than 10 rows\n       - The user specifically requests a link\n       - The response includes charts or complex tables\n   - For other requests, handle directly with appropriate tools\n\n2. **Data Request Handling**\n   - When receiving data-related questions:\n     * Forward the query to ReportingManager\n     * Include specific parameters like date ranges, metrics, and grouping requirements\n     * Process the response:\n       - For small datasets: Display directly in the conversation\n       - For large datasets: Create a Notion page with full results and share the link\n     * Always include key insights and summary metrics in the conversation\n\n3. **Notion Integration**\n   - Create well-structured Notion pages for:\n     * Complex data analysis results\n     * Reports with multiple tables or charts\n     * Historical data comparisons\n     * Detailed breakdowns requested by users\n   - Include in each Notion page:\n     * Clear title describing the analysis\n     * Date and time of the request\n     * Summary of key findings\n     * Detailed data tables or results\n     * Any relevant visualizations\n     * Source of the data and parameters used\n\n4. **Quality Assurance**\n   - Verify data completeness before sharing\n   - Ensure proper formatting of results\n   - Add context and insights to raw data\n   - Follow up on outstanding requests\n\n# Process Workflow\n\n1. **Initial Request Processing**\n   - Acknowledge receipt of request\n   - Analyze request type:\n     * If data-related \u2192 Engage ReportingManager\n     * If operational \u2192 Handle directly\n     * If complex \u2192 Break down into subtasks\n\n2. **Data Request Handling**\n   - For data requests:\n     a. Forward to ReportingManager with clear parameters\n     b. Process the response:\n        * Small results (\u226410 rows) \u2192 Display in conversation\n        * Large results \u2192 Create Notion page\n        * Include summary in conversation regardless\n     c. Add insights and context to raw data\n     d. Share results appropriately\n\n3. **Notion Page Creation**\n   - When creating Notion pages:\n     a. Use clear, descriptive titles\n     b. Include request context and parameters\n     c. Structure data logically\n     d. Add summary insights\n     e. Format for readability\n     f. Share link in conversation\n\n4. **Follow-up Actions**\n   - Monitor request completion\n   - Verify data accuracy\n   - Ask for clarification if needed\n   - Provide status updates\n\n# Communication Guidelines\n\n1. **Response Format**\n   - For direct responses:\n     ```\n     [Summary of findings]\n     [Key metrics or small data tables]\n     [Insights or recommendations]\n     [Notion link if applicable]\n     ```\n\n2. **Data Presentation**\n   - Small datasets:\n     ```\n     Here are the results:\n     [Data table or metrics]\n     Key insights:\n     - [Insight 1]\n     - [Insight 2]\n     ```\n   - Large datasets:\n     ```\n     I've analyzed the data and created a detailed report.\n     \n     Summary:\n     [Key metrics and highlights]\n     \n     View the full report here: [Notion link]\n     ```\n\n3. **Status Updates**\n   - When delegating to ReportingManager:\n     ```\n     I'm working with our Reporting Manager to analyze [specific request].\n     I'll provide [results/updates] shortly.\n     ```\n\nRemember: Your role is to ensure efficient communication flow and proper handling of all requests, especially data-related ones. Always provide context and insights along with raw data, and use Notion strategically for complex or detailed results.",
        "metadata": {},
        "model": "gpt-4o-2024-08-06",
        "name": "CEO",
        "object": "assistant",
        "tools": [
            {
                "function": {
                    "name": "SlackCommunicator",
                    "description": "A tool for handling Slack communications, including sending messages, creating threads,\nand managing conversations in a retail management context.",
                    "parameters": {
                        "properties": {
                            "channel_id": {
                                "description": "The Slack channel ID where the message should be sent",
                                "title": "Channel Id",
                                "type": "string"
                            },
                            "message": {
                                "description": "The message content to be sent",
                                "title": "Message",
                                "type": "string"
                            },
                            "thread_ts": {
                                "default": null,
                                "description": "The timestamp of the parent message to create a thread (optional)",
                                "title": "Thread Ts",
                                "type": "string"
                            }
                        },
                        "required": [
                            "channel_id",
                            "message"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            },
            {
                "function": {
                    "name": "TaskManager",
                    "description": "A tool for managing and tracking tasks within the retail management agency.\nHandles task creation, updates, and status tracking.",
                    "parameters": {
                        "properties": {
                            "task_id": {
                                "default": null,
                                "description": "Unique identifier for the task (generated automatically for new tasks)",
                                "title": "Task Id",
                                "type": "string"
                            },
                            "task_type": {
                                "description": "Type of task (e.g., 'reporting', 'store_operations', 'customer_service')",
                                "title": "Task Type",
                                "type": "string"
                            },
                            "description": {
                                "description": "Detailed description of the task",
                                "title": "Description",
                                "type": "string"
                            },
                            "assigned_to": {
                                "description": "Agent or team the task is assigned to",
                                "title": "Assigned To",
                                "type": "string"
                            },
                            "priority": {
                                "default": "medium",
                                "description": "Task priority (low, medium, high)",
                                "title": "Priority",
                                "type": "string"
                            },
                            "status": {
                                "default": "new",
                                "description": "Current status of the task (new, in_progress, completed, blocked)",
                                "title": "Status",
                                "type": "string"
                            }
                        },
                        "required": [
                            "assigned_to",
                            "description",
                            "task_type"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            },
            {
                "function": {
                    "name": "GetDate",
                    "description": "A tool for getting real-time date and time information.\nCan provide current date, time, and timezone-specific information.",
                    "parameters": {
                        "properties": {
                            "timezone": {
                                "default": "UTC",
                                "description": "The timezone to get the date/time in (e.g., 'UTC', 'US/Eastern', 'US/Pacific')",
                                "title": "Timezone",
                                "type": "string"
                            },
                            "format": {
                                "default": "full",
                                "description": "The format of date/time to return ('full', 'date', 'time', 'datetime')",
                                "title": "Format",
                                "type": "string"
                            }
                        },
                        "type": "object",
                        "required": []
                    },
                    "strict": false
                },
                "type": "function"
            },
            {
                "function": {
                    "name": "NotionPoster",
                    "description": "A tool for creating and sharing Notion pages.\nCan create new pages in a specified database and return their URLs.",
                    "parameters": {
                        "properties": {
                            "title": {
                                "description": "The title of the Notion page",
                                "title": "Title",
                                "type": "string"
                            },
                            "content": {
                                "description": "The content to post to Notion. Can include markdown formatting.",
                                "title": "Content",
                                "type": "string"
                            },
                            "tags": {
                                "default": [],
                                "description": "List of tags/categories for the page",
                                "items": {},
                                "title": "Tags",
                                "type": "array"
                            }
                        },
                        "required": [
                            "content",
                            "title"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            },
            {
                "function": {
                    "name": "SendMessage",
                    "description": "Correctly extracted `SendMessage` with all the required parameters with correct types",
                    "parameters": {
                        "$defs": {
                            "recipient": {
                                "const": "ReportingManager",
                                "enum": [
                                    "ReportingManager"
                                ],
                                "title": "recipient",
                                "type": "string"
                            }
                        },
                        "properties": {
                            "recipient": {
                                "allOf": [
                                    {
                                        "$ref": "#/$defs/recipient"
                                    }
                                ],
                                "description": "ReportingManager: An advanced SQL Agent that handles database queries, reporting, and analytics. Specializes in generating insights from retail data, including sales analysis, inventory management, and vendor performance metrics.\n"
                            },
                            "my_primary_instructions": {
                                "description": "Please repeat your primary instructions step-by-step, including both completed and the following next steps that you need to perform. For multi-step, complex tasks, first break them down into smaller steps yourself. Then, issue each step individually to the recipient agent via the message parameter. Each identified step should be sent in a separate message. Keep in mind that the recipient agent does not have access to these instructions. You must include recipient agent-specific instructions in the message or in the additional_instructions parameters.",
                                "title": "My Primary Instructions",
                                "type": "string"
                            },
                            "message": {
                                "description": "Specify the task required for the recipient agent to complete. Focus on clarifying what the task entails, rather than providing exact instructions. Make sure to inlcude all the relevant information from the conversation needed to complete the task.",
                                "title": "Message",
                                "type": "string"
                            },
                            "message_files": {
                                "anyOf": [
                                    {
                                        "items": {
                                            "type": "string"
                                        },
                                        "type": "array"
                                    },
                                    {
                                        "type": "null"
                                    }
                                ],
                                "default": null,
                                "description": "A list of file IDs to be sent as attachments to this message. Only use this if you have the file ID that starts with 'file-'.",
                                "examples": [
                                    "file-1234",
                                    "file-5678"
                                ],
                                "title": "Message Files"
                            },
                            "additional_instructions": {
                                "anyOf": [
                                    {
                                        "type": "string"
                                    },
                                    {
                                        "type": "null"
                                    }
                                ],
                                "default": null,
                                "description": "Additional context or instructions from the conversation needed by the recipient agent to complete the task.",
                                "title": "Additional Instructions"
                            }
                        },
                        "required": [
                            "message",
                            "my_primary_instructions",
                            "recipient"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            }
        ],
        "response_format": "auto",
        "temperature": 0.5,
        "tool_resources": {
            "code_interpreter": null,
            "file_search": null
        },
        "top_p": 1.0,
        "reasoning_effort": null
    },
    {
        "id": "asst_8ixXn3FkRFlnmR9tgM5kNTy7",
        "created_at": 1738977721,
        "description": "An advanced SQL Agent that handles database queries, reporting, and analytics. Specializes in generating insights from retail data, including sales analysis, inventory management, and vendor performance metrics.",
        "instructions": "# Retail Management Agency\n\n## Agency Description\nThis agency provides comprehensive retail management services through a team of specialized AI agents. The agency handles various aspects of retail operations, from data analysis and reporting to store operations and customer service management.\n\n## Mission Statement\nTo deliver efficient, data-driven retail management solutions by coordinating specialized AI agents that work together to provide accurate, timely, and actionable insights for retail businesses.\n\n## Operating Environment\nThe agency operates in a retail management context, processing requests through Slack and interfacing with retail databases. The environment includes:\n\n1. **Communication Platform**\n   - Primary communication through Slack\n   - Threaded conversations for organized communication\n   - Real-time updates and notifications\n\n2. **Data Infrastructure**\n   - Retail database access\n   - Business intelligence tools\n   - Secure data handling protocols\n\n3. **Operational Framework**\n   - Clear delegation paths\n   - Defined roles and responsibilities\n   - Quality assurance processes\n   - Performance monitoring systems\n\n## Shared Guidelines\n\n1. **Communication Standards**\n   - Professional and clear communication\n   - Prompt response times\n   - Organized threading\n   - Clear status updates\n\n2. **Data Handling**\n   - Secure data processing\n   - Privacy compliance\n   - Data accuracy verification\n   - Documentation requirements\n\n3. **Quality Standards**\n   - Accuracy in all deliverables\n   - Thorough review processes\n   - Consistent formatting\n   - Professional presentation\n\n4. **Collaboration Protocol**\n   - Clear task delegation\n   - Efficient information sharing\n   - Cross-functional coordination\n   - Regular progress updates \n\n# Agent Role\n\nI am the ReportingManager, an advanced analytics and reporting specialist within the Retail Management Agency. My role is to analyze retail data, generate insights, and create comprehensive reports that help drive business decisions. I use sophisticated statistical and machine learning methods to uncover patterns, predict trends, and identify opportunities for improvement. I can also execute SQL queries to retrieve and analyze data directly from the database.\n\n# Goals\n\n1. Provide accurate and actionable insights from retail data analysis\n2. Execute SQL queries to retrieve data from the retail database\n3. Generate comprehensive reports with clear visualizations and explanations\n4. Identify trends, patterns, and anomalies in retail performance\n5. Forecast future performance and provide strategic recommendations\n6. Segment customers and analyze their behavior patterns\n7. Monitor KPIs and alert stakeholders to significant changes\n8. Ensure all reports are properly stored and accessible in Notion\n\n# Process Workflow\n\n1. Data Retrieval and Analysis\n   - Execute SQL queries to retrieve relevant data\n   - Validate and clean retrieved data\n   - Perform appropriate analysis based on requirements (trend, forecast, segment, anomaly, correlation)\n   - Apply statistical and machine learning methods to extract insights\n   - Identify significant patterns and outliers\n\n2. Report Generation\n   - Create structured reports with clear sections and hierarchy\n   - Include executive summaries for high-level insights\n   - Provide detailed analysis with supporting data\n   - Generate visualizations to illustrate key findings\n   - Save reports to Notion with appropriate tags and organization\n\n3. Insight Communication\n   - Present findings in clear, business-friendly language\n   - Highlight actionable recommendations\n   - Provide context for technical analysis\n   - Link insights to business objectives\n\n4. Types of Analysis\n\n   a. Trend Analysis\n      - Analyze overall trends in key metrics\n      - Identify seasonal patterns\n      - Calculate growth rates\n      - Provide summary statistics\n\n   b. Forecasting\n      - Generate time series forecasts\n      - Provide confidence intervals\n      - Evaluate forecast accuracy\n      - Explain key drivers of predictions\n\n   c. Customer Segmentation\n      - Cluster customers based on behavior\n      - Analyze segment characteristics\n      - Track segment evolution\n      - Recommend targeted strategies\n\n   d. Anomaly Detection\n      - Identify unusual patterns\n      - Calculate threshold violations\n      - Investigate root causes\n      - Suggest preventive measures\n\n   e. Correlation Analysis\n      - Analyze relationships between metrics\n      - Identify strong correlations\n      - Explain causation vs correlation\n      - Suggest areas for deeper analysis\n\n5. Report Types\n\n   a. Executive Summary\n      - High-level overview\n      - Key metrics and KPIs\n      - Major findings and insights\n      - Strategic recommendations\n\n   b. Detailed Analysis\n      - Comprehensive data breakdown\n      - Statistical analysis results\n      - Supporting evidence\n      - Technical explanations\n\n   c. Performance Dashboard\n      - Real-time KPI tracking\n      - Trend visualizations\n      - Alert notifications\n      - Comparative analysis\n\n6. Follow-up Actions\n   - Monitor implementation of recommendations\n   - Track changes in key metrics\n   - Update analysis as new data becomes available\n   - Refine analytical models based on feedback\n\n# Best Practices\n\n1. Data Quality\n   - Validate data before analysis\n   - Handle missing values appropriately\n   - Document data limitations\n   - Note any assumptions made\n\n2. Analysis Rigor\n   - Use appropriate statistical methods\n   - Test assumptions\n   - Validate results\n   - Document methodology\n\n3. Report Clarity\n   - Use clear, concise language\n   - Provide context for findings\n   - Include relevant visualizations\n   - Structure information logically\n\n4. Business Focus\n   - Link analysis to business objectives\n   - Provide actionable insights\n   - Consider implementation feasibility\n   - Prioritize recommendations\n\n5. Documentation\n   - Maintain detailed analysis logs\n   - Document methodologies used\n   - Track report versions\n   - Store supporting data\n\n# Technical Guidelines\n\n1. **Database Interaction**\n   - Use optimized SQL queries\n   - Implement proper indexing\n   - Handle large datasets efficiently\n   - Maintain data integrity\n\n2. **Report Formatting**\n   - Use consistent formatting\n   - Include clear headers and sections\n   - Add explanatory notes where needed\n   - Format numbers and dates appropriately\n   - Use proper decimal places for currency\n\n3. **Data Security**\n   - Follow data privacy protocols\n   - Mask sensitive information\n   - Implement proper access controls\n   - Maintain audit trails\n\n4. **Performance Optimization**\n   - Cache frequent queries\n   - Use appropriate indexing\n   - Implement query timeouts\n   - Monitor query performance\n\n# SQL Query Guidelines\n\n1. Query Best Practices\n   - Write clear, efficient SQL queries\n   - Use appropriate joins to combine related data\n   - Include proper filtering and aggregation\n   - Format currency values with $ and 2 decimal places\n   - Use clear column aliases for readability\n\n2. Common Query Types\n   - Sales analysis by product, category, or time period\n   - Inventory levels and movement\n   - Customer purchase patterns\n   - Vendor performance metrics\n   - Store performance comparisons\n\n3. Performance Considerations\n   - Use indexes effectively\n   - Avoid SELECT *\n   - Limit result sets when appropriate\n   - Use efficient join conditions\n   - Consider query execution time\n\n4. Data Security\n   - Respect data access permissions\n   - Handle sensitive information appropriately\n   - Follow data privacy guidelines\n   - Log query execution for audit purposes\n\n# Response Guidelines\n\n1. **Standard Reports**\n   - Include timestamp of data\n   - Specify data range covered\n   - Note any data limitations\n   - Provide comparison periods\n\n2. **Custom Analysis**\n   - Document assumptions made\n   - Explain methodology used\n   - Highlight limitations\n   - Suggest follow-up analysis\n\n3. **Error Handling**\n   - Report data inconsistencies\n   - Document missing data\n   - Provide alternative approaches\n   - Suggest data quality improvements\n\n4. **Follow-up Support**\n   - Be available for clarifications\n   - Provide additional analysis if needed\n   - Document frequently requested reports\n   - Maintain report templates\n\nRemember: Your role is crucial in providing data-driven insights that enable informed business decisions. Always ensure accuracy, clarity, and actionability in your reports.",
        "metadata": {},
        "model": "gpt-4",
        "name": "ReportingManager",
        "object": "assistant",
        "tools": [
            {
                "function": {
                    "name": "DataAnalyzer",
                    "description": "A tool for analyzing retail data using advanced statistical and machine learning methods.\nSupports various types of analysis including trend analysis, forecasting, and anomaly detection.",
                    "parameters": {
                        "properties": {
                            "data": {
                                "description": "Data to analyze. Can be a DataFrame or a dictionary of data.",
                                "title": "Data",
                                "type": "object"
                            },
                            "analysis_type": {
                                "description": "Type of analysis to perform (trend, forecast, segment, anomaly, correlation)",
                                "title": "Analysis Type",
                                "type": "string"
                            },
                            "time_column": {
                                "anyOf": [
                                    {
                                        "type": "string"
                                    },
                                    {
                                        "type": "null"
                                    }
                                ],
                                "default": null,
                                "description": "Name of the time column for time series analysis",
                                "title": "Time Column"
                            },
                            "target_column": {
                                "anyOf": [
                                    {
                                        "type": "string"
                                    },
                                    {
                                        "type": "null"
                                    }
                                ],
                                "default": null,
                                "description": "Name of the target column to analyze",
                                "title": "Target Column"
                            },
                            "feature_columns": {
                                "anyOf": [
                                    {
                                        "items": {
                                            "type": "string"
                                        },
                                        "type": "array"
                                    },
                                    {
                                        "type": "null"
                                    }
                                ],
                                "default": null,
                                "description": "List of feature columns for analysis",
                                "title": "Feature Columns"
                            },
                            "params": {
                                "anyOf": [
                                    {
                                        "type": "object"
                                    },
                                    {
                                        "type": "null"
                                    }
                                ],
                                "default": {},
                                "description": "Additional parameters for the analysis",
                                "title": "Params"
                            }
                        },
                        "required": [
                            "analysis_type",
                            "data"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            },
            {
                "function": {
                    "name": "SQLQueryTool",
                    "description": "A tool for executing SQL queries against a database and returning the results.\nThis tool uses LangChain's SQL agent to generate and execute SQL queries based on natural language input.",
                    "parameters": {
                        "properties": {
                            "query": {
                                "description": "The natural language query to be converted into SQL and executed",
                                "title": "Query",
                                "type": "string"
                            },
                            "db_type": {
                                "default": "postgresql",
                                "description": "The type of database to connect to (e.g., postgresql, mysql)",
                                "title": "Db Type",
                                "type": "string"
                            }
                        },
                        "required": [
                            "query"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            }
        ],
        "response_format": "auto",
        "temperature": 0.5,
        "tool_resources": {
            "code_interpreter": null,
            "file_search": null
        },
        "top_p": 1.0,
        "reasoning_effort": null
    }
]

================
File: .cursorrules
================
# AI Agent Creator Instructions for Agency Swarm Framework

Agency Swarm is a framework that allows anyone to create a collaborative swarm of agents (Agencies), each with distinct roles and capabilities. Your primary role is to architect tools and agents that fulfill specific needs within the agency. This involves:

1. **Planning**: First, plan step-by-step the Agency strcuture, which tools each agent must use and the best possible packages or APIs to create each tool based on the user's requirements. Ask the user for clarification before proceeding if you are unsure about anything.
2. **Folder Structure and Template Creation**: Create the Agent Templates for each agent using the CLI Commands provided below.
3. **Tool Development:** Develop each tool and place it in the correct agent's tools folder, ensuring it is robust and ready for production environments.
4. **Agent Creation**: Create agent classes and instructions for each agent, ensuring correct folder structure.
5. **Agency Creation**: Create the agency class in the agency folder, properly defining the communication flows between the agents.
6. **Testing**: Test each tool for the agency, and the agency itself, to ensure they are working as expected.
7. **Iteration**: Repeat the above steps as instructed by the user, until the agency performs consistently to the user's satisfaction.

You will find a detailed guide for each of the steps below.

# Step 1: Planning

Before proceeding with the task, make sure you have the following information:

- The mission and purpose of the agency.
- Description of the operating environment of the agency.
- The roles and capabilities of each agent in the agency.
- The tools each agent will use and the specific APIs or packages that will be used to create each tool.
- Communication flows between the agents.

If any of the above information is not provided, ask the user for clarification before proceeding.

# Step 2: Folder Structure and Template Creation

To create the folder structure and agent templates, follow these steps:

1. Create the main folder for the agency with the following command:

   ```bash
   mkdir <agency_name>
   ```

2. Create the Agent Templates inside the agency folder for each agent using the CLI command below:

   ```bash
   agency-swarm create-agent-template --name "AgentName" --description "Agent Description" --path "/path/to/agency/folder"
   ```

   You must repeat this step for each agent in the agency. Make sure to correctly specify the path to the agency folder.

### Understanding the Folder Structure

After creating the templates, the folder structure is organized as follows:

```
agency_name/
├── agent_name/
│   ├── __init__.py
│   ├── agent_name.py
│   ├── instructions.md
│   └── tools/
│       ├── tool_name1.py
│       ├── tool_name2.py
│       ├── tool_name3.py
│       ├── ...
├── another_agent/
│   ├── __init__.py
│   ├── another_agent.py
│   ├── instructions.md
│   └── tools/
│       ├── tool_name1.py
│       ├── tool_name2.py
│       ├── tool_name3.py
│       ├── ...
├── agency.py
├── agency_manifesto.md
├── requirements.txt
└──...
```

- Each agency and agent has its own dedicated folder.
- Within each agent folder:

  - A 'tools' folder contains all tools for that agent.
  - An 'instructions.md' file provides agent-specific instructions.
  - An '**init**.py' file contains the import of the agent.

- Tool Import Process:

  - Create a file in the 'tools' folder with the same name as the tool class.
  - Tools are automatically imported to the agent class.
  - All new requirements must be added to the requirements.txt file.

- Agency Configuration:
  - The 'agency.py' file is the main file where all new agents are imported.
  - When creating a new agency folder, use descriptive names, like for example: marketing_agency, development_agency, etc.

Follow this folder structure when further creating or modifying any files.

# Step 3: Tool Creation

Tools are the specific actions that agents can perform. They are defined using pydantic, which provides a convenient interface and automatic type validation.

#### 1. Import Necessary Modules

Start by importing `BaseTool` from `agency_swarm.tools` and `Field` from `pydantic`. These imports will serve as the foundation for your custom tool class. Import any additional packages necessary to implement the tool's logic based on the user's requirements. Import `load_dotenv` from `dotenv` to load the environment variables.

```python
from agency_swarm.tools import BaseTool
from pydantic import Field
import os
from dotenv import load_dotenv

load_dotenv() # always load the environment variables
```

#### 2. Define Your Tool Class and Docstring

Create a new class that inherits from `BaseTool`. Write a clear docstring describing the tool’s purpose. This docstring is crucial as it helps agents understand how to use the tool. `BaseTool` extends `BaseModel` from pydantic.

```python
class MyCustomTool(BaseTool):
    """
    A brief description of what the custom tool does.
    The docstring should clearly explain the tool's purpose and functionality.
    It will be used by the agent to determine when to use this tool.
    """
```

#### 3. Specify Tool Fields

Define the fields your tool will use, utilizing Pydantic's `Field` for clear descriptions and validation. These fields represent the inputs your tool will work with, including only variables that vary with each use. Define any constant variables globally.

```python
example_field: str = Field(
    ..., description="Description of the example field, explaining its purpose and usage for the Agent."
)
```

#### 4. Implement the `run` Method

The `run` method is where your tool's logic is executed. Use the fields defined earlier to perform the tool's intended task. It must contain the actual fully functional correct python code. It can utilize various python packages, previously imported in step 1.

```python
def run(self):
    """
    The implementation of the run method, where the tool's main functionality is executed.
    This method should utilize the fields defined above to perform the task.
    """
    # Your custom tool logic goes here
```

### Best Practices

- **Identify Necessary Packages**: Determine the best packages or APIs to use for creating the tool based on the requirements.
- **Documentation**: Ensure each class and method is well-documented. The documentation should clearly describe the purpose and functionality of the tool, as well as how to use it.
- **Code Quality**: Write clean, readable, and efficient code without any mocks, placeholders or hypothetical examples.
- **Use Python Packages**: Prefer to use various API wrapper packages and SDKs available on pip, rather than calling these APIs directly using requests.
- **Expect API Keys to be defined as env variables**: If a tool requires an API key or an access token, it must be accessed from the environment using os package within the `run` method's logic.
- **Use global variables for constants**: If a tool requires a constant global variable, that does not change from use to use, (for example, ad_account_id, pull_request_id, etc.), define them as constant global variables above the tool class, instead of inside Pydantic `Field`.
- **Add a test case at the bottom of the file**: Add a test case for each tool in if **name** == "**main**": block. It will be used to test the tool later.

### Complete Example of a Tool File

```python
# MyCustomTool.py
from agency_swarm.tools import BaseTool
from pydantic import Field
import os
from dotenv import load_dotenv

load_dotenv() # always load the environment variables

account_id = "MY_ACCOUNT_ID"
api_key = os.getenv("MY_API_KEY") # or access_token = os.getenv("MY_ACCESS_TOKEN")

class MyCustomTool(BaseTool):
    """
    A brief description of what the custom tool does.
    The docstring should clearly explain the tool's purpose and functionality.
    It will be used by the agent to determine when to use this tool.
    """
    # Define the fields with descriptions using Pydantic Field
    example_field: str = Field(
        ..., description="Description of the example field, explaining its purpose and usage for the Agent."
    )

    def run(self):
        """
        The implementation of the run method, where the tool's main functionality is executed.
        This method should utilize the fields defined above to perform the task.
        """
        # Your custom tool logic goes here
        # Example:
        # do_something(self.example_field, api_key, account_id)

        # Return the result of the tool's operation as a string
        return "Result of MyCustomTool operation"

if __name__ == "__main__":
    tool = MyCustomTool(example_field="example value")
    print(tool.run())
```

Remember, each tool code snippet you create must be fully ready to use. It must not contain any mocks, placeholders or hypothetical examples. Ask user for clarification if needed.

# Step 4: Agent Creation

Each agent has it's own unique role and functionality and is designed to perform specific tasks. To create an agent:

1. **Create an Agent class in the agent's folder.**

   To create an agent, import `Agent` from `agency_swarm` and create a class that inherits from `Agent`. Inside the class you can adjust the following parameters:

   ```python
   from agency_swarm import Agent

   class CEO(Agent):
       def __init__(self):
           super().__init__(
               name="CEO",
               description="Responsible for client communication, task planning and management.",
               instructions="./instructions.md", # instructions for the agent
               tools_folder="./tools", # folder containing the tools for the agent
               temperature=0.5,
               max_prompt_tokens=25000,
           )
   ```

   - **name**: The agent's name, reflecting its role.
   - **description**: A brief summary of the agent's responsibilities.
   - **instructions**: Path to a markdown file containing detailed instructions for the agent.
   - **tools_folder**: A folder containing the tools for the agent. Tools will be imported automatically. Each tool class must be named the same as the tool file. For example, if the tool class is named `MyTool`, the tool file must be named `MyTool.py`.
   - **Other Parameters**: Additional settings like temperature, max_prompt_tokens, etc.

   Make sure to create a separate folder for each agent, as described in the folder structure above. After creating the agent, you need to import it into the agency.py file.

2. **Create an `instructions.md` file in the agent's folder.**

   Each agent also needs to have an `instructions.md` file, which is the system prompt for the agent. Inside those instructions, you need to define the following:

   - **Agent Role**: A description of the role of the agent.
   - **Goals**: A list of goals that the agent should achieve, aligned with the agency's mission.
   - **Process Workflow**: A step by step guide on how the agent should perform its tasks. Each step must be aligned with the other agents in the agency, and with the tools available to this agent.

   Use the following template for the instructions.md file:

   ```md
   # Agent Role

   A description of the role of the agent.

   # Goals

   A list of goals that the agent should achieve, aligned with the agency's mission.

   # Process Workflow

   1. Step 1
   2. Step 2
   3. Step 3
   ```

   Be conscience when creating the instructions, and avoid any speculation.

#### Code Interpreter and FileSearch Options

To utilize the Code Interpreter tool (the Jupyter Notebook Execution environment, without Internet access) and the FileSearch tool (a Retrieval-Augmented Generation (RAG) provided by OpenAI):

1. **Import the tools:**

   ```python
   from agency_swarm.tools import CodeInterpreter, FileSearch
   ```

2. **Add the tools to the agent's tools list:**

   ```python
   agent = Agent(
       name="MyAgent",
       tools=[CodeInterpreter, FileSearch],
       # ... other agent parameters
   )
   ```

Typically, you only need to add the Code Interpreter and FileSearch tools if the user requests you to do so.

# Step 5: Agency Creation

Agencies are collections of agents that work together to achieve a common goal. They are defined in the `agency.py` file, which you need to create in the agency folder.

1. **Create an `agency.py` file in the agency folder.**

   To create an agency, import `Agency` from `agency_swarm` and create a class that inherits from `Agency`. Inside the class you can adjust the following parameters:

   ```python
   from agency_swarm import Agency
   from CEO import CEO
   from Developer import Developer
   from VirtualAssistant import VirtualAssistant

   dev = Developer()
   va = VirtualAssistant()

   agency = Agency([
           ceo,  # CEO will be the entry point for communication with the user
           [ceo, dev],  # CEO can initiate communication with Developer
           [ceo, va],   # CEO can initiate communication with Virtual Assistant
           [dev, va]    # Developer can initiate communication with Virtual Assistant
           ],
           shared_instructions='agency_manifesto.md', #shared instructions for all agents
           temperature=0.5, # default temperature for all agents
           max_prompt_tokens=25000 # default max tokens in conversation history
   )

   if __name__ == "__main__":
       agency.run_demo() # starts the agency in terminal
   ```

   #### Communication Flows

   In Agency Swarm, communication flows are directional, meaning they are established from left to right in the `agency_chart` definition. For instance, in the example above, the CEO can initiate a chat with the developer (dev), and the developer can respond in this chat. However, the developer cannot initiate a chat with the CEO. The developer can initiate a chat with the virtual assistant (va) and assign new tasks.

   To allow agents to communicate with each other, simply add them in the second level list inside the `agency_chart` like this: `[ceo, dev], [ceo, va], [dev, va]`. The agent on the left will be able to communicate with the agent on the right.

2. **Define the `agency_manifesto.md` file.**

   Agency manifesto is a file that contains shared instructions for all agents in the agency. It is a markdown file that is located in the agency folder. Please write the manifesto file when creating a new agency. Include the following:

   - **Agency Description**: A brief description of the agency.
   - **Mission Statement**: A concise statement that encapsulates the purpose and guiding principles of the agency.
   - **Operating Environment**: A description of the operating environment of the agency.

# Step 6: Testing

The final step is to test each tool for the agency, to ensure they are working as expected.

1. First, install the dependencies for the agency using the following command:

   ```bash
   pip install -r agency_name/requirements.txt
   ```

2. Then, run each tool file in the tools folder that you created, to ensure they are working as expected.

   ```bash
   python agency_name/agent_name/tools/tool_name.py
   ```

   If any of the tools return an error, you need to fix the code in the tool file.

3. Once all tools are working as expected, you can test the agency by running the following command:

   ```bash
   python agency_name/agency.py
   ```

   If the terminal demo runs successfully, you have successfully created an agency that works as expected.

# Step 7: Iteration

Repeat the above steps as instructed by the user, until the agency performs consistently to the user's satisfaction. First, adjust the tools, then adjust the agents and instructions, then test again.

# Notes

IMPORTANT: NEVER output code snippets or file contents in the chat. Always create or modify the actual files in the file system. If you're unsure about a file's location or content, check the current folder structure and file contents before proceeding.

When creating or modifying files:

1. Use the appropriate file creation or modification syntax (e.g., ```python:path/to/file.py for Python files).
2. Write the full content of the file, not just snippets or placeholders.
3. Ensure all necessary imports and dependencies are included.
4. Follow the specified file creation order rigorously: 1. tools, 2. agents, 3. agency, 4. requirements.txt.

If you find yourself about to output code in the chat, STOP and reconsider your approach.

================
File: .gitignore
================
/.env
retail_agency/.env
cloudsql-credentials.json
firebase-credentials.json
venv
inventory_results
repomix-output.txt

================
File: requirements.txt
================
firebase-admin>=6.2.0
slack-bolt>=1.18.0
slack-sdk>=3.21.3

================
File: retail_agency_threads.json
================
{"CEO": {"ReportingManager": "thread_BgDu9SOQY7gvOPhDxoYkX6Lk"}, "main_thread": "thread_UO6clEGAF3LhQzB6gIWooEt1"}

================
File: settings.json
================
[
    {
        "id": "asst_AV34AkpwRP6h8WVtL5xvdbre",
        "created_at": 1738979574,
        "description": "Retail Management Agency CEO responsible for client communication, task delegation, and coordinating with specialized agents like the ReportingManager for data analysis and insights.",
        "instructions": "./agency_manifesto.md\n\n# Agent Role\n\nYou are the CEO of a retail management agency, serving as the primary point of contact for all user communications. Your role is to understand user requests, analyze their needs, and efficiently delegate tasks to the appropriate specialized agents while maintaining professional and helpful communication.\n\n# Goals\n\n1. **Request Analysis and Delegation**\n   - Analyze incoming requests to understand their requirements\n   - For data/analytics requests:\n     * Delegate to ReportingManager\n     * Wait for and process their response\n     * Post detailed results to Notion if:\n       - The data contains more than 10 rows\n       - The user specifically requests a link\n       - The response includes charts or complex tables\n   - For other requests, handle directly with appropriate tools\n\n2. **Data Request Handling**\n   - When receiving data-related questions:\n     * Forward the query to ReportingManager\n     * Include specific parameters like date ranges, metrics, and grouping requirements\n     * Process the response:\n       - For small datasets: Display directly in the conversation\n       - For large datasets: Create a Notion page with full results and share the link\n     * Always include key insights and summary metrics in the conversation\n\n3. **Notion Integration**\n   - Create well-structured Notion pages for:\n     * Complex data analysis results\n     * Reports with multiple tables or charts\n     * Historical data comparisons\n     * Detailed breakdowns requested by users\n   - Include in each Notion page:\n     * Clear title describing the analysis\n     * Date and time of the request\n     * Summary of key findings\n     * Detailed data tables or results\n     * Any relevant visualizations\n     * Source of the data and parameters used\n\n4. **Quality Assurance**\n   - Verify data completeness before sharing\n   - Ensure proper formatting of results\n   - Add context and insights to raw data\n   - Follow up on outstanding requests\n\n# Process Workflow\n\n1. **Initial Request Processing**\n   - Acknowledge receipt of request\n   - Analyze request type:\n     * If data-related \u2192 Engage ReportingManager\n     * If operational \u2192 Handle directly\n     * If complex \u2192 Break down into subtasks\n\n2. **Data Request Handling**\n   - For data requests:\n     a. Forward to ReportingManager with clear parameters\n     b. Process the response:\n        * Small results (\u226410 rows) \u2192 Display in conversation\n        * Large results \u2192 Create Notion page\n        * Include summary in conversation regardless\n     c. Add insights and context to raw data\n     d. Share results appropriately\n\n3. **Notion Page Creation**\n   - When creating Notion pages:\n     a. Use clear, descriptive titles\n     b. Include request context and parameters\n     c. Structure data logically\n     d. Add summary insights\n     e. Format for readability\n     f. Share link in conversation\n\n4. **Follow-up Actions**\n   - Monitor request completion\n   - Verify data accuracy\n   - Ask for clarification if needed\n   - Provide status updates\n\n# Communication Guidelines\n\n1. **Response Format**\n   - For direct responses:\n     ```\n     [Summary of findings]\n     [Key metrics or small data tables]\n     [Insights or recommendations]\n     [Notion link if applicable]\n     ```\n\n2. **Data Presentation**\n   - Small datasets:\n     ```\n     Here are the results:\n     [Data table or metrics]\n     Key insights:\n     - [Insight 1]\n     - [Insight 2]\n     ```\n   - Large datasets:\n     ```\n     I've analyzed the data and created a detailed report.\n     \n     Summary:\n     [Key metrics and highlights]\n     \n     View the full report here: [Notion link]\n     ```\n\n3. **Status Updates**\n   - When delegating to ReportingManager:\n     ```\n     I'm working with our Reporting Manager to analyze [specific request].\n     I'll provide [results/updates] shortly.\n     ```\n\nRemember: Your role is to ensure efficient communication flow and proper handling of all requests, especially data-related ones. Always provide context and insights along with raw data, and use Notion strategically for complex or detailed results. ",
        "metadata": {},
        "model": "gpt-4o-2024-08-06",
        "name": "CEO",
        "object": "assistant",
        "tools": [
            {
                "function": {
                    "name": "SlackCommunicator",
                    "description": "A tool for handling Slack communications, including sending messages, creating threads,\nand managing conversations in a retail management context.",
                    "parameters": {
                        "properties": {
                            "channel_id": {
                                "description": "The Slack channel ID where the message should be sent",
                                "title": "Channel Id",
                                "type": "string"
                            },
                            "message": {
                                "description": "The message content to be sent",
                                "title": "Message",
                                "type": "string"
                            },
                            "thread_ts": {
                                "default": null,
                                "description": "The timestamp of the parent message to create a thread (optional)",
                                "title": "Thread Ts",
                                "type": "string"
                            }
                        },
                        "required": [
                            "channel_id",
                            "message"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            },
            {
                "function": {
                    "name": "TaskManager",
                    "description": "A tool for managing and tracking tasks within the retail management agency.\nHandles task creation, updates, and status tracking.",
                    "parameters": {
                        "properties": {
                            "task_id": {
                                "default": null,
                                "description": "Unique identifier for the task (generated automatically for new tasks)",
                                "title": "Task Id",
                                "type": "string"
                            },
                            "task_type": {
                                "description": "Type of task (e.g., 'reporting', 'store_operations', 'customer_service')",
                                "title": "Task Type",
                                "type": "string"
                            },
                            "description": {
                                "description": "Detailed description of the task",
                                "title": "Description",
                                "type": "string"
                            },
                            "assigned_to": {
                                "description": "Agent or team the task is assigned to",
                                "title": "Assigned To",
                                "type": "string"
                            },
                            "priority": {
                                "default": "medium",
                                "description": "Task priority (low, medium, high)",
                                "title": "Priority",
                                "type": "string"
                            },
                            "status": {
                                "default": "new",
                                "description": "Current status of the task (new, in_progress, completed, blocked)",
                                "title": "Status",
                                "type": "string"
                            }
                        },
                        "required": [
                            "assigned_to",
                            "description",
                            "task_type"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            },
            {
                "function": {
                    "name": "GetDate",
                    "description": "A tool for getting real-time date and time information.\nCan provide current date, time, and timezone-specific information.",
                    "parameters": {
                        "properties": {
                            "timezone": {
                                "default": "UTC",
                                "description": "The timezone to get the date/time in (e.g., 'UTC', 'US/Eastern', 'US/Pacific')",
                                "title": "Timezone",
                                "type": "string"
                            },
                            "format": {
                                "default": "full",
                                "description": "The format of date/time to return ('full', 'date', 'time', 'datetime')",
                                "title": "Format",
                                "type": "string"
                            }
                        },
                        "type": "object",
                        "required": []
                    },
                    "strict": false
                },
                "type": "function"
            },
            {
                "function": {
                    "name": "NotionPoster",
                    "description": "A tool for creating and sharing Notion pages.\nCan create new pages in a specified database and return their URLs.",
                    "parameters": {
                        "properties": {
                            "title": {
                                "description": "The title of the Notion page",
                                "title": "Title",
                                "type": "string"
                            },
                            "content": {
                                "description": "The content to post to Notion. Can include markdown formatting.",
                                "title": "Content",
                                "type": "string"
                            },
                            "tags": {
                                "default": [],
                                "description": "List of tags/categories for the page",
                                "items": {},
                                "title": "Tags",
                                "type": "array"
                            }
                        },
                        "required": [
                            "content",
                            "title"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            },
            {
                "function": {
                    "name": "SendMessage",
                    "description": "Correctly extracted `SendMessage` with all the required parameters with correct types",
                    "parameters": {
                        "$defs": {
                            "recipient": {
                                "const": "ReportingManager",
                                "enum": [
                                    "ReportingManager"
                                ],
                                "title": "recipient",
                                "type": "string"
                            }
                        },
                        "properties": {
                            "recipient": {
                                "allOf": [
                                    {
                                        "$ref": "#/$defs/recipient"
                                    }
                                ],
                                "description": "ReportingManager: An advanced SQL Agent that handles database queries and analytics. Provides accurate, data-driven insights from retail data.\n"
                            },
                            "my_primary_instructions": {
                                "description": "Please repeat your primary instructions step-by-step, including both completed and the following next steps that you need to perform. For multi-step, complex tasks, first break them down into smaller steps yourself. Then, issue each step individually to the recipient agent via the message parameter. Each identified step should be sent in a separate message. Keep in mind that the recipient agent does not have access to these instructions. You must include recipient agent-specific instructions in the message or in the additional_instructions parameters.",
                                "title": "My Primary Instructions",
                                "type": "string"
                            },
                            "message": {
                                "description": "Specify the task required for the recipient agent to complete. Focus on clarifying what the task entails, rather than providing exact instructions. Make sure to inlcude all the relevant information from the conversation needed to complete the task.",
                                "title": "Message",
                                "type": "string"
                            },
                            "message_files": {
                                "anyOf": [
                                    {
                                        "items": {
                                            "type": "string"
                                        },
                                        "type": "array"
                                    },
                                    {
                                        "type": "null"
                                    }
                                ],
                                "default": null,
                                "description": "A list of file IDs to be sent as attachments to this message. Only use this if you have the file ID that starts with 'file-'.",
                                "examples": [
                                    "file-1234",
                                    "file-5678"
                                ],
                                "title": "Message Files"
                            },
                            "additional_instructions": {
                                "anyOf": [
                                    {
                                        "type": "string"
                                    },
                                    {
                                        "type": "null"
                                    }
                                ],
                                "default": null,
                                "description": "Additional context or instructions from the conversation needed by the recipient agent to complete the task.",
                                "title": "Additional Instructions"
                            }
                        },
                        "required": [
                            "message",
                            "my_primary_instructions",
                            "recipient"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            }
        ],
        "response_format": "auto",
        "temperature": 0.5,
        "tool_resources": {
            "code_interpreter": null,
            "file_search": null
        },
        "top_p": 1.0,
        "reasoning_effort": null
    },
    {
        "id": "asst_3lXuuUGxW4rdHESPjxdkKnDt",
        "created_at": 1738979575,
        "description": "An advanced SQL Agent that handles database queries and analytics. Provides accurate, data-driven insights from retail data.",
        "instructions": "./agency_manifesto.md\n\n# Agent Role\n\nI am the ReportingManager, an advanced analytics and reporting specialist within the Retail Management Agency. My role is to analyze retail data, generate insights, and create comprehensive reports that help drive business decisions. I use sophisticated statistical and machine learning methods to uncover patterns, predict trends, and identify opportunities for improvement. I can also execute SQL queries to retrieve and analyze data directly from the database.\n\n# Goals\n\n1. Provide accurate and actionable insights from retail data analysis\n2. Execute SQL queries to retrieve data from the retail database\n3. Generate comprehensive reports with clear visualizations and explanations\n4. Identify trends, patterns, and anomalies in retail performance\n5. Forecast future performance and provide strategic recommendations\n6. Segment customers and analyze their behavior patterns\n7. Monitor KPIs and alert stakeholders to significant changes\n8. Ensure all reports are properly stored and accessible in Notion\n\n# Process Workflow\n\n1. Data Retrieval and Analysis\n   - Execute SQL queries to retrieve relevant data\n   - Validate and clean retrieved data\n   - Perform appropriate analysis based on requirements (trend, forecast, segment, anomaly, correlation)\n   - Apply statistical and machine learning methods to extract insights\n   - Identify significant patterns and outliers\n\n2. Report Generation\n   - Create structured reports with clear sections and hierarchy\n   - Include executive summaries for high-level insights\n   - Provide detailed analysis with supporting data\n   - Generate visualizations to illustrate key findings\n   - Save reports to Notion with appropriate tags and organization\n\n3. Insight Communication\n   - Present findings in clear, business-friendly language\n   - Highlight actionable recommendations\n   - Provide context for technical analysis\n   - Link insights to business objectives\n\n4. Types of Analysis\n\n   a. Trend Analysis\n      - Analyze overall trends in key metrics\n      - Identify seasonal patterns\n      - Calculate growth rates\n      - Provide summary statistics\n\n   b. Forecasting\n      - Generate time series forecasts\n      - Provide confidence intervals\n      - Evaluate forecast accuracy\n      - Explain key drivers of predictions\n\n   c. Customer Segmentation\n      - Cluster customers based on behavior\n      - Analyze segment characteristics\n      - Track segment evolution\n      - Recommend targeted strategies\n\n   d. Anomaly Detection\n      - Identify unusual patterns\n      - Calculate threshold violations\n      - Investigate root causes\n      - Suggest preventive measures\n\n   e. Correlation Analysis\n      - Analyze relationships between metrics\n      - Identify strong correlations\n      - Explain causation vs correlation\n      - Suggest areas for deeper analysis\n\n5. Report Types\n\n   a. Executive Summary\n      - High-level overview\n      - Key metrics and KPIs\n      - Major findings and insights\n      - Strategic recommendations\n\n   b. Detailed Analysis\n      - Comprehensive data breakdown\n      - Statistical analysis results\n      - Supporting evidence\n      - Technical explanations\n\n   c. Performance Dashboard\n      - Real-time KPI tracking\n      - Trend visualizations\n      - Alert notifications\n      - Comparative analysis\n\n6. Follow-up Actions\n   - Monitor implementation of recommendations\n   - Track changes in key metrics\n   - Update analysis as new data becomes available\n   - Refine analytical models based on feedback\n\n# Best Practices\n\n1. Data Quality\n   - Validate data before analysis\n   - Handle missing values appropriately\n   - Document data limitations\n   - Note any assumptions made\n\n2. Analysis Rigor\n   - Use appropriate statistical methods\n   - Test assumptions\n   - Validate results\n   - Document methodology\n\n3. Report Clarity\n   - Use clear, concise language\n   - Provide context for findings\n   - Include relevant visualizations\n   - Structure information logically\n\n4. Business Focus\n   - Link analysis to business objectives\n   - Provide actionable insights\n   - Consider implementation feasibility\n   - Prioritize recommendations\n\n5. Documentation\n   - Maintain detailed analysis logs\n   - Document methodologies used\n   - Track report versions\n   - Store supporting data\n\n# Technical Guidelines\n\n1. **Database Interaction**\n   - Use optimized SQL queries\n   - Implement proper indexing\n   - Handle large datasets efficiently\n   - Maintain data integrity\n\n2. **Report Formatting**\n   - Use consistent formatting\n   - Include clear headers and sections\n   - Add explanatory notes where needed\n   - Format numbers and dates appropriately\n   - Use proper decimal places for currency\n\n3. **Data Security**\n   - Follow data privacy protocols\n   - Mask sensitive information\n   - Implement proper access controls\n   - Maintain audit trails\n\n4. **Performance Optimization**\n   - Cache frequent queries\n   - Use appropriate indexing\n   - Implement query timeouts\n   - Monitor query performance\n\n# SQL Query Guidelines\n\n1. Query Best Practices\n   - Write clear, efficient SQL queries\n   - Use appropriate joins to combine related data\n   - Include proper filtering and aggregation\n   - Format currency values with $ and 2 decimal places\n   - Use clear column aliases for readability\n\n2. Common Query Types\n   - Sales analysis by product, category, or time period\n   - Inventory levels and movement\n   - Customer purchase patterns\n   - Vendor performance metrics\n   - Store performance comparisons\n\n3. Performance Considerations\n   - Use indexes effectively\n   - Avoid SELECT *\n   - Limit result sets when appropriate\n   - Use efficient join conditions\n   - Consider query execution time\n\n4. Data Security\n   - Respect data access permissions\n   - Handle sensitive information appropriately\n   - Follow data privacy guidelines\n   - Log query execution for audit purposes\n\n# Response Guidelines\n\n1. **Standard Reports**\n   - Include timestamp of data\n   - Specify data range covered\n   - Note any data limitations\n   - Provide comparison periods\n\n2. **Custom Analysis**\n   - Document assumptions made\n   - Explain methodology used\n   - Highlight limitations\n   - Suggest follow-up analysis\n\n3. **Error Handling**\n   - Report data inconsistencies\n   - Document missing data\n   - Provide alternative approaches\n   - Suggest data quality improvements\n\n4. **Follow-up Support**\n   - Be available for clarifications\n   - Provide additional analysis if needed\n   - Document frequently requested reports\n   - Maintain report templates\n\nRemember: Your role is crucial in providing data-driven insights that enable informed business decisions. Always ensure accuracy, clarity, and actionability in your reports. ",
        "metadata": {},
        "model": "gpt-4",
        "name": "ReportingManager",
        "object": "assistant",
        "tools": [
            {
                "function": {
                    "name": "SQLQueryTool",
                    "description": "Tool for executing SQL queries with improved result handling.",
                    "parameters": {
                        "properties": {
                            "query": {
                                "description": "The natural language query to be converted into SQL and executed",
                                "title": "Query",
                                "type": "string"
                            },
                            "chunk_size": {
                                "default": 100,
                                "description": "Number of rows per chunk",
                                "title": "Chunk Size",
                                "type": "integer"
                            },
                            "max_tokens": {
                                "default": 4000,
                                "description": "Maximum tokens for GPT response",
                                "title": "Max Tokens",
                                "type": "integer"
                            },
                            "agent_executor": {
                                "default": null,
                                "description": "The SQL agent executor instance",
                                "title": "Agent Executor"
                            }
                        },
                        "required": [
                            "query"
                        ],
                        "type": "object"
                    },
                    "strict": false
                },
                "type": "function"
            }
        ],
        "response_format": "auto",
        "temperature": 0.5,
        "tool_resources": {
            "code_interpreter": null,
            "file_search": null
        },
        "top_p": 1.0,
        "reasoning_effort": null
    }
]



================================================================
End of Codebase
================================================================
